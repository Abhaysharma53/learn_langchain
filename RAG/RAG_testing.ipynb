{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f88c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q youtube-transcript-api \\\n",
    "#  faiss-cpu tiktoken python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312ae1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\n",
      "ERROR: No matching distribution found for faiss\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5f90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement faiss==1.5.3 (from versions: none)\n",
      "ERROR: No matching distribution found for faiss==1.5.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install faiss==1.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f82c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from youtube_transcript_api import TranscriptsDisabled, YouTubeTranscriptApi\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "769e9cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "204e5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytt_api = YouTubeTranscriptApi()\n",
    "raw_transcript = ytt_api.get_transcript('ngX3kRdP33E', languages= ['en', 'hi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09e015f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Okay, Google just released its agent to',\n",
       "  'start': 0.08,\n",
       "  'duration': 5.04},\n",
       " {'text': 'aagent protocol, but with its launch,',\n",
       "  'start': 2.159,\n",
       "  'duration': 4.961},\n",
       " {'text': 'there are some big questions which arise',\n",
       "  'start': 5.12,\n",
       "  'duration': 5.04},\n",
       " {'text': 'like what about MCP? Like does A2A',\n",
       "  'start': 7.12,\n",
       "  'duration': 6.639},\n",
       " {'text': 'replace MCP? Is there an overlap? And if',\n",
       "  'start': 10.16,\n",
       "  'duration': 6.64},\n",
       " {'text': 'not, when do you use what? Well, in this',\n",
       "  'start': 13.759,\n",
       "  'duration': 5.121},\n",
       " {'text': \"stream, we're going to pit A2A against\",\n",
       "  'start': 16.8,\n",
       "  'duration': 4.639},\n",
       " {'text': 'MCP and see what their differences are.',\n",
       "  'start': 18.88,\n",
       "  'duration': 6.08},\n",
       " {'text': 'Take a good example to highlight some',\n",
       "  'start': 21.439,\n",
       "  'duration': 5.76},\n",
       " {'text': 'use cases and finally arrive at a',\n",
       "  'start': 24.96,\n",
       "  'duration': 4.159},\n",
       " {'text': 'framework which helps us decide when to',\n",
       "  'start': 27.199,\n",
       "  'duration': 4.801},\n",
       " {'text': \"use what. Let's get started. I think the\",\n",
       "  'start': 29.119,\n",
       "  'duration': 6.241},\n",
       " {'text': 'best way to compare any frameworks is by',\n",
       "  'start': 32.0,\n",
       "  'duration': 7.04},\n",
       " {'text': 'looking at an example, right? And we',\n",
       "  'start': 35.36,\n",
       "  'duration': 5.6},\n",
       " {'text': \"already saw this, so let's just go\",\n",
       "  'start': 39.04,\n",
       "  'duration': 3.199},\n",
       " {'text': 'through it again. It can be a quick',\n",
       "  'start': 40.96,\n",
       "  'duration': 3.759},\n",
       " {'text': \"recap. Okay, let's say that I'm a human.\",\n",
       "  'start': 42.239,\n",
       "  'duration': 4.8},\n",
       " {'text': \"Yeah, a cute little guy. That's me. Yep.\",\n",
       "  'start': 44.719,\n",
       "  'duration': 4.561},\n",
       " {'text': \"Right here. And let's say I'm supposed\",\n",
       "  'start': 47.039,\n",
       "  'duration': 3.68},\n",
       " {'text': 'to finish a project by the end of the',\n",
       "  'start': 49.28,\n",
       "  'duration': 3.68},\n",
       " {'text': 'week, right? And I keep all my project',\n",
       "  'start': 50.719,\n",
       "  'duration': 4.401},\n",
       " {'text': 'stuff in notion and I keep all my',\n",
       "  'start': 52.96,\n",
       "  'duration': 4.4},\n",
       " {'text': 'calendar in Google calendar, right? So',\n",
       "  'start': 55.12,\n",
       "  'duration': 4.079},\n",
       " {'text': 'my projects and my tasks are in notion',\n",
       "  'start': 57.36,\n",
       "  'duration': 3.28},\n",
       " {'text': 'and everything else is in Google',\n",
       "  'start': 59.199,\n",
       "  'duration': 3.921},\n",
       " {'text': 'calendar. And I just want to make sure',\n",
       "  'start': 60.64,\n",
       "  'duration': 4.0},\n",
       " {'text': 'that all my tasks for a particular',\n",
       "  'start': 63.12,\n",
       "  'duration': 2.96},\n",
       " {'text': 'project gets done by the end of the',\n",
       "  'start': 64.64,\n",
       "  'duration': 5.12},\n",
       " {'text': 'week. Why? Cuz my manager is annoying.',\n",
       "  'start': 66.08,\n",
       "  'duration': 5.28},\n",
       " {'text': 'So yeah, need to finish it by the end of',\n",
       "  'start': 69.76,\n",
       "  'duration': 3.28},\n",
       " {'text': 'the week. And we start with a request.',\n",
       "  'start': 71.36,\n",
       "  'duration': 3.2},\n",
       " {'text': 'We tell our personal assistant that hey',\n",
       "  'start': 73.04,\n",
       "  'duration': 2.88},\n",
       " {'text': 'you know what schedule all the task for',\n",
       "  'start': 74.56,\n",
       "  'duration': 3.199},\n",
       " {'text': 'project A by the end of this week. Now',\n",
       "  'start': 75.92,\n",
       "  'duration': 3.519},\n",
       " {'text': 'the assistant will go ahead to notion',\n",
       "  'start': 77.759,\n",
       "  'duration': 2.881},\n",
       " {'text': 'and will be like hey give me all the',\n",
       "  'start': 79.439,\n",
       "  'duration': 3.201},\n",
       " {'text': 'task for this project with estimates.',\n",
       "  'start': 80.64,\n",
       "  'duration': 4.479},\n",
       " {'text': 'Notion will give me back a response. It',\n",
       "  'start': 82.64,\n",
       "  'duration': 4.32},\n",
       " {'text': 'will say that hey you have two task task',\n",
       "  'start': 85.119,\n",
       "  'duration': 4.64},\n",
       " {'text': 'A is 1 hour task B is 4 hours. Then the',\n",
       "  'start': 86.96,\n",
       "  'duration': 4.479},\n",
       " {'text': 'assistant goes ahead to Google calendar',\n",
       "  'start': 89.759,\n",
       "  'duration': 3.36},\n",
       " {'text': 'and goes like hey you know what can you',\n",
       "  'start': 91.439,\n",
       "  'duration': 3.841},\n",
       " {'text': 'now schedule these tasks in this week',\n",
       "  'start': 93.119,\n",
       "  'duration': 3.841},\n",
       " {'text': 'itself. I have two task just be kind to',\n",
       "  'start': 95.28,\n",
       "  'duration': 3.839},\n",
       " {'text': 'me. So obviously Google calendar tries',\n",
       "  'start': 96.96,\n",
       "  'duration': 5.6},\n",
       " {'text': 'its best but unfortunately you know we',\n",
       "  'start': 99.119,\n",
       "  'duration': 5.68},\n",
       " {'text': \"don't have enough space in our calendar.\",\n",
       "  'start': 102.56,\n",
       "  'duration': 4.0},\n",
       " {'text': \"We just don't have a 4-hour block free.\",\n",
       "  'start': 104.799,\n",
       "  'duration': 3.081},\n",
       " {'text': \"I'm a busy man\", 'start': 106.56, 'duration': 4.16},\n",
       " {'text': 'guys. So yeah we get a response back.',\n",
       "  'start': 107.88,\n",
       "  'duration': 5.08},\n",
       " {'text': \"Now what do we do? So let's say the\",\n",
       "  'start': 110.72,\n",
       "  'duration': 4.8},\n",
       " {'text': 'assistant takes a call that hey can we',\n",
       "  'start': 112.96,\n",
       "  'duration': 4.72},\n",
       " {'text': 'break task B down? So instead of',\n",
       "  'start': 115.52,\n",
       "  'duration': 3.68},\n",
       " {'text': 'scheduling it in one shot can we break',\n",
       "  'start': 117.68,\n",
       "  'duration': 3.2},\n",
       " {'text': 'it down into subtasks? So we go to',\n",
       "  'start': 119.2,\n",
       "  'duration': 3.84},\n",
       " {'text': 'notion and we say hey can we break this',\n",
       "  'start': 120.88,\n",
       "  'duration': 4.16},\n",
       " {'text': 'task further down? and notion goes like,',\n",
       "  'start': 123.04,\n",
       "  'duration': 4.16},\n",
       " {'text': '\"Oh, you know what? We actually can.\"',\n",
       "  'start': 125.04,\n",
       "  'duration': 4.079},\n",
       " {'text': 'And then it gives a list of subtask or',\n",
       "  'start': 127.2,\n",
       "  'duration': 3.279},\n",
       " {'text': 'something as well. And then we go ahead',\n",
       "  'start': 129.119,\n",
       "  'duration': 3.041},\n",
       " {'text': 'to the calendar and be like, \"Hey, can',\n",
       "  'start': 130.479,\n",
       "  'duration': 2.961},\n",
       " {'text': 'you schedule this instead?\" And it says,',\n",
       "  'start': 132.16,\n",
       "  'duration': 3.12},\n",
       " {'text': '\"Hey, I scheduled it.\" And while all of',\n",
       "  'start': 133.44,\n",
       "  'duration': 4.159},\n",
       " {'text': 'this was happening, because our agent',\n",
       "  'start': 135.28,\n",
       "  'duration': 5.52},\n",
       " {'text': 'looks like this, our agent is impatient.',\n",
       "  'start': 137.599,\n",
       "  'duration': 4.72},\n",
       " {'text': 'And you know, in my previous video,',\n",
       "  'start': 140.8,\n",
       "  'duration': 3.2},\n",
       " {'text': \"users are monkeys. They're impatient\",\n",
       "  'start': 142.319,\n",
       "  'duration': 3.521},\n",
       " {'text': 'beings. So, you need to make sure, you',\n",
       "  'start': 144.0,\n",
       "  'duration': 3.04},\n",
       " {'text': 'need to comfort them that, hey,',\n",
       "  'start': 145.84,\n",
       "  'duration': 2.56},\n",
       " {'text': 'something is happening. So, they stick',\n",
       "  'start': 147.04,\n",
       "  'duration': 2.559},\n",
       " {'text': 'along. You know, they just stay with',\n",
       "  'start': 148.4,\n",
       "  'duration': 2.72},\n",
       " {'text': \"you. They don't leave you. So what you\",\n",
       "  'start': 149.599,\n",
       "  'duration': 3.841},\n",
       " {'text': 'do is as things are happening in this',\n",
       "  'start': 151.12,\n",
       "  'duration': 4.199},\n",
       " {'text': 'agent, you need to start giving it',\n",
       "  'start': 153.44,\n",
       "  'duration': 3.92},\n",
       " {'text': 'updates, right? You need to give it',\n",
       "  'start': 155.319,\n",
       "  'duration': 3.56},\n",
       " {'text': 'updates to build a good user experience',\n",
       "  'start': 157.36,\n",
       "  'duration': 3.12},\n",
       " {'text': 'that hey, you know what? I got two tasks',\n",
       "  'start': 158.879,\n",
       "  'duration': 3.841},\n",
       " {'text': 'back. Oh, wait a minute. Or maybe you',\n",
       "  'start': 160.48,\n",
       "  'duration': 3.44},\n",
       " {'text': \"could have an update saying now I'm\",\n",
       "  'start': 162.72,\n",
       "  'duration': 2.4},\n",
       " {'text': 'trying to schedule it. Maybe there could',\n",
       "  'start': 163.92,\n",
       "  'duration': 2.72},\n",
       " {'text': 'be another update to schedule it. Maybe',\n",
       "  'start': 165.12,\n",
       "  'duration': 3.04},\n",
       " {'text': 'we should add that. Trying to schedule',\n",
       "  'start': 166.64,\n",
       "  'duration': 3.44},\n",
       " {'text': 'now. Uh this should come before',\n",
       "  'start': 168.16,\n",
       "  'duration': 3.84},\n",
       " {'text': \"scheduling. Don't worry about the timing\",\n",
       "  'start': 170.08,\n",
       "  'duration': 3.36},\n",
       " {'text': \"in the sequence diagram. It's all over\",\n",
       "  'start': 172.0,\n",
       "  'duration': 2.8},\n",
       " {'text': 'the place. Okay, but you get the gist,',\n",
       "  'start': 173.44,\n",
       "  'duration': 3.519},\n",
       " {'text': \"right? You're giving your user updates\",\n",
       "  'start': 174.8,\n",
       "  'duration': 3.84},\n",
       " {'text': 'on what is happening. So the user is in',\n",
       "  'start': 176.959,\n",
       "  'duration': 3.681},\n",
       " {'text': 'loop. This will help the user know that',\n",
       "  'start': 178.64,\n",
       "  'duration': 3.519},\n",
       " {'text': 'okay my agent is actually doing the',\n",
       "  'start': 180.64,\n",
       "  'duration': 2.8},\n",
       " {'text': 'right thing which is kind of important',\n",
       "  'start': 182.159,\n",
       "  'duration': 3.281},\n",
       " {'text': 'and not just that I mean you could even',\n",
       "  'start': 183.44,\n",
       "  'duration': 3.6},\n",
       " {'text': 'have some human and loop scenarios right',\n",
       "  'start': 185.44,\n",
       "  'duration': 3.92},\n",
       " {'text': 'where you could be like okay we want to',\n",
       "  'start': 187.04,\n",
       "  'duration': 5.04},\n",
       " {'text': 'break a task down and notion was like',\n",
       "  'start': 189.36,\n",
       "  'duration': 4.56},\n",
       " {'text': 'yeah you can break this task down but',\n",
       "  'start': 192.08,\n",
       "  'duration': 3.6},\n",
       " {'text': \"what if notion doesn't know how to break\",\n",
       "  'start': 193.92,\n",
       "  'duration': 3.84},\n",
       " {'text': 'the task down what if it comes up with',\n",
       "  'start': 195.68,\n",
       "  'duration': 4.639},\n",
       " {'text': 'incorrect subtasks maybe we want to show',\n",
       "  'start': 197.76,\n",
       "  'duration': 4.96},\n",
       " {'text': 'the user an input hey we want to break',\n",
       "  'start': 200.319,\n",
       "  'duration': 5.28},\n",
       " {'text': 'task B down can we do that and the user',\n",
       "  'start': 202.72,\n",
       "  'duration': 4.64},\n",
       " {'text': 'goes like yeah cool you can do this and',\n",
       "  'start': 205.599,\n",
       "  'duration': 3.681},\n",
       " {'text': 'then you schedule it so You have like a',\n",
       "  'start': 207.36,\n",
       "  'duration': 3.84},\n",
       " {'text': 'a little human in loop scenario going on',\n",
       "  'start': 209.28,\n",
       "  'duration': 3.28},\n",
       " {'text': \"in here. That's how the real world\",\n",
       "  'start': 211.2,\n",
       "  'duration': 3.52},\n",
       " {'text': 'operates, right? Your agents might screw',\n",
       "  'start': 212.56,\n",
       "  'duration': 3.679},\n",
       " {'text': 'things up or some things might not be',\n",
       "  'start': 214.72,\n",
       "  'duration': 2.879},\n",
       " {'text': 'possible. So, you need like follow-up',\n",
       "  'start': 216.239,\n",
       "  'duration': 3.201},\n",
       " {'text': 'questions. And then you also want to',\n",
       "  'start': 217.599,\n",
       "  'duration': 3.681},\n",
       " {'text': 'give feedback to the user. You also want',\n",
       "  'start': 219.44,\n",
       "  'duration': 3.439},\n",
       " {'text': 'like human and loop scenarios and all',\n",
       "  'start': 221.28,\n",
       "  'duration': 3.679},\n",
       " {'text': \"that funny stuff. That's how real agent\",\n",
       "  'start': 222.879,\n",
       "  'duration': 4.08},\n",
       " {'text': 'interaction works, at least in the real',\n",
       "  'start': 224.959,\n",
       "  'duration': 3.761},\n",
       " {'text': \"world, right? This is the example we're\",\n",
       "  'start': 226.959,\n",
       "  'duration': 3.36},\n",
       " {'text': 'going to use to start comparing things.',\n",
       "  'start': 228.72,\n",
       "  'duration': 3.92},\n",
       " {'text': 'Sounds good. You guys are with me? Okay.',\n",
       "  'start': 230.319,\n",
       "  'duration': 3.601},\n",
       " {'text': \"One important point. I'm going to skip\",\n",
       "  'start': 232.64,\n",
       "  'duration': 2.8},\n",
       " {'text': \"some lowhanging fruits like I'm going to\",\n",
       "  'start': 233.92,\n",
       "  'duration': 3.92},\n",
       " {'text': \"skip o authentication. I'm going to skip\",\n",
       "  'start': 235.44,\n",
       "  'duration': 4.24},\n",
       " {'text': 'service discovery. I know for some',\n",
       "  'start': 237.84,\n",
       "  'duration': 3.679},\n",
       " {'text': 'people this was like a big deal that oh',\n",
       "  'start': 239.68,\n",
       "  'duration': 4.16},\n",
       " {'text': \"my god MCP doesn't support O or MCP\",\n",
       "  'start': 241.519,\n",
       "  'duration': 3.92},\n",
       " {'text': \"doesn't have a service discovery API or\",\n",
       "  'start': 243.84,\n",
       "  'duration': 3.119},\n",
       " {'text': 'whatever. Those are not that big',\n",
       "  'start': 245.439,\n",
       "  'duration': 3.44},\n",
       " {'text': 'problems because I mean you could just',\n",
       "  'start': 246.959,\n",
       "  'duration': 3.601},\n",
       " {'text': 'shove in tokens in the headers and',\n",
       "  'start': 248.879,\n",
       "  'duration': 3.041},\n",
       " {'text': 'things like that like you do with REST',\n",
       "  'start': 250.56,\n",
       "  'duration': 3.2},\n",
       " {'text': 'APIs anyways. Those were not that big',\n",
       "  'start': 251.92,\n",
       "  'duration': 3.12},\n",
       " {'text': 'problems at least according to me. So',\n",
       "  'start': 253.76,\n",
       "  'duration': 2.879},\n",
       " {'text': \"we're going to skip it just you know for\",\n",
       "  'start': 255.04,\n",
       "  'duration': 3.28},\n",
       " {'text': 'clarity sake. A2A does support',\n",
       "  'start': 256.639,\n",
       "  'duration': 3.44},\n",
       " {'text': 'authentication and authorization and in',\n",
       "  'start': 258.32,\n",
       "  'duration': 3.92},\n",
       " {'text': 'fact it just uses the open API',\n",
       "  'start': 260.079,\n",
       "  'duration': 4.801},\n",
       " {'text': 'authentication. again open API not open',\n",
       "  'start': 262.24,\n",
       "  'duration': 5.6},\n",
       " {'text': \"AI it's open API so it uses the open\",\n",
       "  'start': 264.88,\n",
       "  'duration': 5.84},\n",
       " {'text': \"API's authentication specification which\",\n",
       "  'start': 267.84,\n",
       "  'duration': 4.32},\n",
       " {'text': 'essentially says the agent will',\n",
       "  'start': 270.72,\n",
       "  'duration': 3.84},\n",
       " {'text': 'explicitly mention that hey I support',\n",
       "  'start': 272.16,\n",
       "  'duration': 3.92},\n",
       " {'text': 'basic authentication which is username',\n",
       "  'start': 274.56,\n",
       "  'duration': 2.8},\n",
       " {'text': 'password I support better token',\n",
       "  'start': 276.08,\n",
       "  'duration': 3.28},\n",
       " {'text': 'authentication I support O2 blah blah',\n",
       "  'start': 277.36,\n",
       "  'duration': 3.68},\n",
       " {'text': 'blah so the agent will be explicit about',\n",
       "  'start': 279.36,\n",
       "  'duration': 3.68},\n",
       " {'text': 'what authentication it supports but you',\n",
       "  'start': 281.04,\n",
       "  'duration': 4.159},\n",
       " {'text': 'still like have to give it up front I',\n",
       "  'start': 283.04,\n",
       "  'duration': 4.719},\n",
       " {'text': \"know this is on uh MCP's road map as\",\n",
       "  'start': 285.199,\n",
       "  'duration': 5.041},\n",
       " {'text': 'well not just that A2A also has a',\n",
       "  'start': 287.759,\n",
       "  'duration': 4.88},\n",
       " {'text': 'discovery document so it actually says',\n",
       "  'start': 290.24,\n",
       "  'duration': 3.84},\n",
       " {'text': 'says that oh you know what if you want',\n",
       "  'start': 292.639,\n",
       "  'duration': 3.601},\n",
       " {'text': 'to discover agents you can do it using',\n",
       "  'start': 294.08,\n",
       "  'duration': 4.24},\n",
       " {'text': 'well-known format which we usually use',\n",
       "  'start': 296.24,\n",
       "  'duration': 3.92},\n",
       " {'text': 'like sitemaps and things like that you',\n",
       "  'start': 298.32,\n",
       "  'duration': 4.4},\n",
       " {'text': 'could also have like an API they clearly',\n",
       "  'start': 300.16,\n",
       "  'duration': 4.08},\n",
       " {'text': \"mentioned that hey we don't have a\",\n",
       "  'start': 302.72,\n",
       "  'duration': 3.919},\n",
       " {'text': 'protocol for the registry yet but if you',\n",
       "  'start': 304.24,\n",
       "  'duration': 4.56},\n",
       " {'text': 'need it then let us know because I agree',\n",
       "  'start': 306.639,\n",
       "  'duration': 3.441},\n",
       " {'text': 'this is not that important you can make',\n",
       "  'start': 308.8,\n",
       "  'duration': 2.72},\n",
       " {'text': 'it yourself why do you need a protocol',\n",
       "  'start': 310.08,\n",
       "  'duration': 3.04},\n",
       " {'text': \"for these things but okay that's the\",\n",
       "  'start': 311.52,\n",
       "  'duration': 3.519},\n",
       " {'text': 'point these are lowhanging fruits these',\n",
       "  'start': 313.12,\n",
       "  'duration': 4.0},\n",
       " {'text': 'are things which do not really impact',\n",
       "  'start': 315.039,\n",
       "  'duration': 3.44},\n",
       " {'text': \"functionality that much I mean they're\",\n",
       "  'start': 317.12,\n",
       "  'duration': 3.04},\n",
       " {'text': 'important you have to implement them but',\n",
       "  'start': 318.479,\n",
       "  'duration': 4.16},\n",
       " {'text': \"I'm going to skim those things so Let's\",\n",
       "  'start': 320.16,\n",
       "  'duration': 4.64},\n",
       " {'text': 'talk about the comparison. And how are',\n",
       "  'start': 322.639,\n",
       "  'duration': 3.84},\n",
       " {'text': \"we going to do that? We're going to make\",\n",
       "  'start': 324.8,\n",
       "  'duration': 3.679},\n",
       " {'text': \"a table. Let's try to do this. I think\",\n",
       "  'start': 326.479,\n",
       "  'duration': 3.601},\n",
       " {'text': 'this is where we should start. Okay. So,',\n",
       "  'start': 328.479,\n",
       "  'duration': 3.201},\n",
       " {'text': \"the way we're going to do this is we're\",\n",
       "  'start': 330.08,\n",
       "  'duration': 3.36},\n",
       " {'text': \"going to write A2A here. We're going to\",\n",
       "  'start': 331.68,\n",
       "  'duration': 4.799},\n",
       " {'text': 'write MCP here. And then each feature',\n",
       "  'start': 333.44,\n",
       "  'duration': 4.56},\n",
       " {'text': \"we'll write here. So, against every\",\n",
       "  'start': 336.479,\n",
       "  'duration': 3.601},\n",
       " {'text': \"feature, we'll see what A2A does for it,\",\n",
       "  'start': 338.0,\n",
       "  'duration': 4.56},\n",
       " {'text': 'what MCP does for it, right? Cuz on the',\n",
       "  'start': 340.08,\n",
       "  'duration': 3.92},\n",
       " {'text': \"surface, it seems like there's a lot of\",\n",
       "  'start': 342.56,\n",
       "  'duration': 3.12},\n",
       " {'text': \"overlap, but it's actually not. Spoiler\",\n",
       "  'start': 344.0,\n",
       "  'duration': 3.039},\n",
       " {'text': \"alert, they're very different protocols\",\n",
       "  'start': 345.68,\n",
       "  'duration': 3.359},\n",
       " {'text': 'for very different purposes. Yeah. Cool.',\n",
       "  'start': 347.039,\n",
       "  'duration': 3.681},\n",
       " {'text': 'So the first thing I want to start with',\n",
       "  'start': 349.039,\n",
       "  'duration': 4.401},\n",
       " {'text': 'is capability discovery. So how do you',\n",
       "  'start': 350.72,\n",
       "  'duration': 4.64},\n",
       " {'text': 'discover what is the possible options or',\n",
       "  'start': 353.44,\n",
       "  'duration': 3.28},\n",
       " {'text': 'what are the different functionalities',\n",
       "  'start': 355.36,\n",
       "  'duration': 3.6},\n",
       " {'text': 'which an A2A server or an MCP server',\n",
       "  'start': 356.72,\n",
       "  'duration': 5.039},\n",
       " {'text': 'give you right like how did you know in',\n",
       "  'start': 358.96,\n",
       "  'duration': 5.519},\n",
       " {'text': 'this example that hey you can get these',\n",
       "  'start': 361.759,\n",
       "  'duration': 5.201},\n",
       " {'text': 'tasks like you can even ask it for task',\n",
       "  'start': 364.479,\n",
       "  'duration': 4.321},\n",
       " {'text': 'while asking for task you can also ask',\n",
       "  'start': 366.96,\n",
       "  'duration': 3.28},\n",
       " {'text': 'for estimates how do you know those',\n",
       "  'start': 368.8,\n",
       "  'duration': 3.44},\n",
       " {'text': 'things right so there has to be some way',\n",
       "  'start': 370.24,\n",
       "  'duration': 4.48},\n",
       " {'text': 'to discover what capabilities or what',\n",
       "  'start': 372.24,\n",
       "  'duration': 4.88},\n",
       " {'text': 'functionality does this guy give us',\n",
       "  'start': 374.72,\n",
       "  'duration': 4.24},\n",
       " {'text': \"that's that's kind of important yeah so\",\n",
       "  'start': 377.12,\n",
       "  'duration': 3.44},\n",
       " {'text': 'for a toa we have something called',\n",
       "  'start': 378.96,\n",
       "  'duration': 2.959},\n",
       " {'text': 'skills the way we discover cap',\n",
       "  'start': 380.56,\n",
       "  'duration': 3.199},\n",
       " {'text': 'capabilities in A2A is through skills.',\n",
       "  'start': 381.919,\n",
       "  'duration': 3.84},\n",
       " {'text': 'Way we do it in MCP is through tools.',\n",
       "  'start': 383.759,\n",
       "  'duration': 4.72},\n",
       " {'text': 'Now, technically MCP does have resources',\n",
       "  'start': 385.759,\n",
       "  'duration': 4.481},\n",
       " {'text': \"and prompts as well. I'm just going to\",\n",
       "  'start': 388.479,\n",
       "  'duration': 3.28},\n",
       " {'text': \"skip those guys. I don't know who's\",\n",
       "  'start': 390.24,\n",
       "  'duration': 4.079},\n",
       " {'text': \"using it, but uh yeah, we're just going\",\n",
       "  'start': 391.759,\n",
       "  'duration': 4.961},\n",
       " {'text': 'to limit our thing to tools and skills',\n",
       "  'start': 394.319,\n",
       "  'duration': 3.6},\n",
       " {'text': \"because I think that's where a lot of\",\n",
       "  'start': 396.72,\n",
       "  'duration': 2.8},\n",
       " {'text': 'the mixup would happen. Prompts and',\n",
       "  'start': 397.919,\n",
       "  'duration': 3.521},\n",
       " {'text': 'resources is like a completely different',\n",
       "  'start': 399.52,\n",
       "  'duration': 3.6},\n",
       " {'text': \"thing. So, I'm just going to skip those\",\n",
       "  'start': 401.44,\n",
       "  'duration': 4.16},\n",
       " {'text': 'things. So, in A2A, you have the concept',\n",
       "  'start': 403.12,\n",
       "  'duration': 4.16},\n",
       " {'text': 'of a skill. In MCP, you have a concept',\n",
       "  'start': 405.6,\n",
       "  'duration': 3.68},\n",
       " {'text': 'of a tool. So we already saw an example',\n",
       "  'start': 407.28,\n",
       "  'duration': 4.72},\n",
       " {'text': \"for A2A but let's go through that again.\",\n",
       "  'start': 409.28,\n",
       "  'duration': 4.24},\n",
       " {'text': \"Let's just look at discovery first and\",\n",
       "  'start': 412.0,\n",
       "  'duration': 3.44},\n",
       " {'text': 'then we can say who can use it right. So',\n",
       "  'start': 413.52,\n",
       "  'duration': 3.84},\n",
       " {'text': 'the first how it works for A2A. You have',\n",
       "  'start': 415.44,\n",
       "  'duration': 3.68},\n",
       " {'text': 'something called the agent card and this',\n",
       "  'start': 417.36,\n",
       "  'duration': 3.839},\n",
       " {'text': 'agent card is on steroids. This is good',\n",
       "  'start': 419.12,\n",
       "  'duration': 4.72},\n",
       " {'text': 'stuff. This is what you can discover. So',\n",
       "  'start': 421.199,\n",
       "  'duration': 4.641},\n",
       " {'text': \"every agent publishes this card. It's\",\n",
       "  'start': 423.84,\n",
       "  'duration': 4.0},\n",
       " {'text': 'just a JSON object and it declares that',\n",
       "  'start': 425.84,\n",
       "  'duration': 4.88},\n",
       " {'text': 'hey this is my capability right. So you',\n",
       "  'start': 427.84,\n",
       "  'duration': 5.12},\n",
       " {'text': 'can clearly see that okay as input it',\n",
       "  'start': 430.72,\n",
       "  'duration': 4.479},\n",
       " {'text': 'supports plain text and as output it can',\n",
       "  'start': 432.96,\n",
       "  'duration': 4.239},\n",
       " {'text': 'do plain text as well and it can even',\n",
       "  'start': 435.199,\n",
       "  'duration': 3.601},\n",
       " {'text': 'create HTML code as output. In',\n",
       "  'start': 437.199,\n",
       "  'duration': 3.201},\n",
       " {'text': 'capabilities it says it can support',\n",
       "  'start': 438.8,\n",
       "  'duration': 2.88},\n",
       " {'text': 'streaming. So it supports streaming',\n",
       "  'start': 440.4,\n",
       "  'duration': 3.04},\n",
       " {'text': 'responses. It supports something called',\n",
       "  'start': 441.68,\n",
       "  'duration': 3.28},\n",
       " {'text': \"push notifications. We won't talk about\",\n",
       "  'start': 443.44,\n",
       "  'duration': 2.96},\n",
       " {'text': 'push notifications. See the previous',\n",
       "  'start': 444.96,\n",
       "  'duration': 3.04},\n",
       " {'text': 'video for that one. Now this is the most',\n",
       "  'start': 446.4,\n",
       "  'duration': 2.799},\n",
       " {'text': \"important thing. Oh there's also one\",\n",
       "  'start': 448.0,\n",
       "  'duration': 2.16},\n",
       " {'text': \"more thing. There's something called\",\n",
       "  'start': 449.199,\n",
       "  'duration': 2.801},\n",
       " {'text': 'task state updates or something like',\n",
       "  'start': 450.16,\n",
       "  'duration': 3.039},\n",
       " {'text': \"that. That's also a very cool\", 'start': 452.0, 'duration': 2.88},\n",
       " {'text': 'capability. So there are three',\n",
       "  'start': 453.199,\n",
       "  'duration': 3.201},\n",
       " {'text': 'capabilities right now in the spec.',\n",
       "  'start': 454.88,\n",
       "  'duration': 3.12},\n",
       " {'text': 'Again check out the previous video for',\n",
       "  'start': 456.4,\n",
       "  'duration': 3.6},\n",
       " {'text': 'all this. Now this is the cool stuff.',\n",
       "  'start': 458.0,\n",
       "  'duration': 5.36},\n",
       " {'text': 'Unlike MCP, A2A has skills, right? This',\n",
       "  'start': 460.0,\n",
       "  'duration': 5.039},\n",
       " {'text': 'is a Google maps agent, right? So the',\n",
       "  'start': 463.36,\n",
       "  'duration': 3.52},\n",
       " {'text': 'skill is route planning. It can do route',\n",
       "  'start': 465.039,\n",
       "  'duration': 3.44},\n",
       " {'text': 'planning. It helps planning routes',\n",
       "  'start': 466.88,\n",
       "  'duration': 3.28},\n",
       " {'text': 'between two locations. Now this is a',\n",
       "  'start': 468.479,\n",
       "  'duration': 3.44},\n",
       " {'text': 'very generic description. What does it',\n",
       "  'start': 470.16,\n",
       "  'duration': 3.36},\n",
       " {'text': \"actually mean? It's got examples right\",\n",
       "  'start': 471.919,\n",
       "  'duration': 3.521},\n",
       " {'text': 'here which says plan my route from',\n",
       "  'start': 473.52,\n",
       "  'duration': 4.079},\n",
       " {'text': 'Sunnyale to Mountain View. Oh, so route',\n",
       "  'start': 475.44,\n",
       "  'duration': 3.68},\n",
       " {'text': 'planning means actually planning my',\n",
       "  'start': 477.599,\n",
       "  'duration': 3.04},\n",
       " {'text': 'route from Sunnyale to Mountain View.',\n",
       "  'start': 479.12,\n",
       "  'duration': 3.04},\n",
       " {'text': 'Okay, interesting. What does route',\n",
       "  'start': 480.639,\n",
       "  'duration': 3.28},\n",
       " {'text': \"planning mean? What's the commute time\",\n",
       "  'start': 482.16,\n",
       "  'duration': 4.0},\n",
       " {'text': 'from Sunnyale to San Francisco at 9:00',\n",
       "  'start': 483.919,\n",
       "  'duration': 4.56},\n",
       " {'text': \"a.m.? Oh, it's being so specific here.\",\n",
       "  'start': 486.16,\n",
       "  'duration': 4.719},\n",
       " {'text': 'So, not only can I use it to plan my',\n",
       "  'start': 488.479,\n",
       "  'duration': 4.56},\n",
       " {'text': 'routes, I can also use it to get the',\n",
       "  'start': 490.879,\n",
       "  'duration': 4.081},\n",
       " {'text': 'time for the routes. Like, how much time',\n",
       "  'start': 493.039,\n",
       "  'duration': 3.6},\n",
       " {'text': 'does it take? Maybe I know the route. I',\n",
       "  'start': 494.96,\n",
       "  'duration': 2.88},\n",
       " {'text': 'just want to know how much time would it',\n",
       "  'start': 496.639,\n",
       "  'duration': 3.041},\n",
       " {'text': 'take for a particular hour, something',\n",
       "  'start': 497.84,\n",
       "  'duration': 3.68},\n",
       " {'text': 'like that. You can do that, too. All',\n",
       "  'start': 499.68,\n",
       "  'duration': 3.519},\n",
       " {'text': \"right, cool. I'm learning new things.\",\n",
       "  'start': 501.52,\n",
       "  'duration': 3.119},\n",
       " {'text': 'And then you can get turnbyturn',\n",
       "  'start': 503.199,\n",
       "  'duration': 3.601},\n",
       " {'text': 'directions. So, one route could be just,',\n",
       "  'start': 504.639,\n",
       "  'duration': 4.0},\n",
       " {'text': 'hey, here to here. And maybe another',\n",
       "  'start': 506.8,\n",
       "  'duration': 3.519},\n",
       " {'text': 'could be a turnbyturn route where you',\n",
       "  'start': 508.639,\n",
       "  'duration': 3.041},\n",
       " {'text': 'get like a very comprehensive thing',\n",
       "  'start': 510.319,\n",
       "  'duration': 2.801},\n",
       " {'text': 'like, okay, go left from here, go right',\n",
       "  'start': 511.68,\n",
       "  'duration': 3.76},\n",
       " {'text': \"from here. The point is it's actually\",\n",
       "  'start': 513.12,\n",
       "  'duration': 5.76},\n",
       " {'text': 'telling you very high level broad',\n",
       "  'start': 515.44,\n",
       "  'duration': 5.44},\n",
       " {'text': 'capabilities or queries you can ask this',\n",
       "  'start': 518.88,\n",
       "  'duration': 3.92},\n",
       " {'text': \"agent right and it's very natural\",\n",
       "  'start': 520.88,\n",
       "  'duration': 3.68},\n",
       " {'text': \"language focused I think that's a really\",\n",
       "  'start': 522.8,\n",
       "  'duration': 3.28},\n",
       " {'text': \"important point again I don't know how\",\n",
       "  'start': 524.56,\n",
       "  'duration': 3.52},\n",
       " {'text': \"to best describe this I'm so sorry about\",\n",
       "  'start': 526.08,\n",
       "  'duration': 5.16},\n",
       " {'text': 'that but in A2A skills are really broad',\n",
       "  'start': 528.08,\n",
       "  'duration': 5.6},\n",
       " {'text': 'capabilities with some examples on how',\n",
       "  'start': 531.24,\n",
       "  'duration': 4.36},\n",
       " {'text': 'you can mess around with it there is',\n",
       "  'start': 533.68,\n",
       "  'duration': 4.159},\n",
       " {'text': 'intelligence on the back end which is',\n",
       "  'start': 535.6,\n",
       "  'duration': 4.56},\n",
       " {'text': 'trying its best to take your query and',\n",
       "  'start': 537.839,\n",
       "  'duration': 4.321},\n",
       " {'text': 'give you an answer on it so under the',\n",
       "  'start': 540.16,\n",
       "  'duration': 5.2},\n",
       " {'text': 'hood technically speaking this one thing',\n",
       "  'start': 542.16,\n",
       "  'duration': 6.0},\n",
       " {'text': 'could require 10 tool calls maybe. I',\n",
       "  'start': 545.36,\n",
       "  'duration': 4.32},\n",
       " {'text': \"don't know how many tool calls would\",\n",
       "  'start': 548.16,\n",
       "  'duration': 3.52},\n",
       " {'text': 'Google have to do to answer this. I',\n",
       "  'start': 549.68,\n",
       "  'duration': 3.12},\n",
       " {'text': \"don't know how many tool calls would\",\n",
       "  'start': 551.68,\n",
       "  'duration': 2.64},\n",
       " {'text': 'Google have to do to answer that. The',\n",
       "  'start': 552.8,\n",
       "  'duration': 3.12},\n",
       " {'text': 'point is I just give it questions in',\n",
       "  'start': 554.32,\n",
       "  'duration': 2.88},\n",
       " {'text': \"natural language and I don't have to\",\n",
       "  'start': 555.92,\n",
       "  'duration': 2.64},\n",
       " {'text': 'worry about the breaking down of',\n",
       "  'start': 557.2,\n",
       "  'duration': 2.56},\n",
       " {'text': 'questions and things like that. I can',\n",
       "  'start': 558.56,\n",
       "  'duration': 2.719},\n",
       " {'text': 'just give it questions and give me some',\n",
       "  'start': 559.76,\n",
       "  'duration': 3.199},\n",
       " {'text': 'example questions as well. Thank you.',\n",
       "  'start': 561.279,\n",
       "  'duration': 2.961},\n",
       " {'text': 'And obviously you can have as many',\n",
       "  'start': 562.959,\n",
       "  'duration': 3.201},\n",
       " {'text': \"skills as you want. So that's how you\",\n",
       "  'start': 564.24,\n",
       "  'duration': 4.56},\n",
       " {'text': 'define skills in A2A at least. Okay. In',\n",
       "  'start': 566.16,\n",
       "  'duration': 4.72},\n",
       " {'text': 'MCP you do tools. So how would it look',\n",
       "  'start': 568.8,\n",
       "  'duration': 5.039},\n",
       " {'text': 'like in MCP?', 'start': 570.88, 'duration': 5.68},\n",
       " {'text': 'Okay, so this is how you do it in MCP.',\n",
       "  'start': 573.839,\n",
       "  'duration': 5.841},\n",
       " {'text': \"Oh boy. So you define tools. There's a\",\n",
       "  'start': 576.56,\n",
       "  'duration': 4.959},\n",
       " {'text': 'very fixed scope of what a tool is. So',\n",
       "  'start': 579.68,\n",
       "  'duration': 3.599},\n",
       " {'text': 'in this example, the tool is calculate',\n",
       "  'start': 581.519,\n",
       "  'duration': 3.361},\n",
       " {'text': \"sum. There's a description. There's a\",\n",
       "  'start': 583.279,\n",
       "  'duration': 3.841},\n",
       " {'text': 'very strict input that hey for the input',\n",
       "  'start': 584.88,\n",
       "  'duration': 3.519},\n",
       " {'text': 'you have to give me a number. You have',\n",
       "  'start': 587.12,\n",
       "  'duration': 3.04},\n",
       " {'text': 'to give me another number and you know',\n",
       "  'start': 588.399,\n",
       "  'duration': 3.041},\n",
       " {'text': \"both are required. I mean it doesn't\",\n",
       "  'start': 590.16,\n",
       "  'duration': 2.799},\n",
       " {'text': 'describe the output but it describes the',\n",
       "  'start': 591.44,\n",
       "  'duration': 4.079},\n",
       " {'text': \"input. It's very very fixed right. And\",\n",
       "  'start': 592.959,\n",
       "  'duration': 4.481},\n",
       " {'text': 'all you can do is you get an API saying',\n",
       "  'start': 595.519,\n",
       "  'duration': 3.361},\n",
       " {'text': 'give me all the tools and that will',\n",
       "  'start': 597.44,\n",
       "  'duration': 2.8},\n",
       " {'text': 'essentially give you an array of these',\n",
       "  'start': 598.88,\n",
       "  'duration': 2.8},\n",
       " {'text': \"tools. So you could see there's a tool\",\n",
       "  'start': 600.24,\n",
       "  'duration': 2.88},\n",
       " {'text': 'for calculate sum there could be a tool',\n",
       "  'start': 601.68,\n",
       "  'duration': 3.12},\n",
       " {'text': 'for add two numbers it could be for',\n",
       "  'start': 603.12,\n",
       "  'duration': 3.2},\n",
       " {'text': 'everything. So again going back to this',\n",
       "  'start': 604.8,\n",
       "  'duration': 3.52},\n",
       " {'text': 'example which we had here if this was an',\n",
       "  'start': 606.32,\n",
       "  'duration': 4.72},\n",
       " {'text': 'MCP server instead of notion agent. So',\n",
       "  'start': 608.32,\n",
       "  'duration': 4.32},\n",
       " {'text': 'instead of getting skills back with',\n",
       "  'start': 611.04,\n",
       "  'duration': 3.68},\n",
       " {'text': 'example queries you can ask you would',\n",
       "  'start': 612.64,\n",
       "  'duration': 3.92},\n",
       " {'text': 'get a list of tools back and one tool',\n",
       "  'start': 614.72,\n",
       "  'duration': 3.92},\n",
       " {'text': 'would be get tasks with estimates. One',\n",
       "  'start': 616.56,\n",
       "  'duration': 4.16},\n",
       " {'text': 'tool could be just get tasks right one',\n",
       "  'start': 618.64,\n",
       "  'duration': 4.72},\n",
       " {'text': 'tool could be get estimate for task',\n",
       "  'start': 620.72,\n",
       "  'duration': 4.72},\n",
       " {'text': 'right. So those would be tools. You',\n",
       "  'start': 623.36,\n",
       "  'duration': 4.08},\n",
       " {'text': 'would have five six different tools and',\n",
       "  'start': 625.44,\n",
       "  'duration': 3.36},\n",
       " {'text': 'somebody would have to decide what tool',\n",
       "  'start': 627.44,\n",
       "  'duration': 4.24},\n",
       " {'text': 'to call. A2A is not explicit at all. A2A',\n",
       "  'start': 628.8,\n",
       "  'duration': 4.56},\n",
       " {'text': 'just gives you a list of skills and you',\n",
       "  'start': 631.68,\n",
       "  'duration': 3.44},\n",
       " {'text': \"just give it a query. You don't even\",\n",
       "  'start': 633.36,\n",
       "  'duration': 3.2},\n",
       " {'text': 'have to say whether I want to use skill',\n",
       "  'start': 635.12,\n",
       "  'duration': 3.04},\n",
       " {'text': 'one or skill 2. You just give it a query',\n",
       "  'start': 636.56,\n",
       "  'duration': 3.04},\n",
       " {'text': 'and the agent will figure everything out',\n",
       "  'start': 638.16,\n",
       "  'duration': 3.919},\n",
       " {'text': \"by itself. That's wild, right? In MCP\",\n",
       "  'start': 639.6,\n",
       "  'duration': 4.32},\n",
       " {'text': 'very well defined in A28 is a little bit',\n",
       "  'start': 642.079,\n",
       "  'duration': 4.32},\n",
       " {'text': 'more broad. Right? Some clarifications.',\n",
       "  'start': 643.92,\n",
       "  'duration': 5.12},\n",
       " {'text': \"I don't want to make you think that wait\",\n",
       "  'start': 646.399,\n",
       "  'duration': 5.361},\n",
       " {'text': 'that A2A is more broad and MCP cannot be',\n",
       "  'start': 649.04,\n",
       "  'duration': 4.72},\n",
       " {'text': \"broad. MCP can be broad as well. Let's\",\n",
       "  'start': 651.76,\n",
       "  'duration': 3.6},\n",
       " {'text': 'just uh drill down on this a little bit',\n",
       "  'start': 653.76,\n",
       "  'duration': 3.84},\n",
       " {'text': \"more. Okay. So basically what I'm\",\n",
       "  'start': 655.36,\n",
       "  'duration': 4.64},\n",
       " {'text': 'suggesting is in MCP you have like get',\n",
       "  'start': 657.6,\n",
       "  'duration': 5.28},\n",
       " {'text': 'tasks right and then for get tax you',\n",
       "  'start': 660.0,\n",
       "  'duration': 4.56},\n",
       " {'text': 'could take a project as an input and you',\n",
       "  'start': 662.88,\n",
       "  'duration': 3.44},\n",
       " {'text': \"give some output right that's an example\",\n",
       "  'start': 664.56,\n",
       "  'duration': 3.279},\n",
       " {'text': 'of how the tool would look like and',\n",
       "  'start': 666.32,\n",
       "  'duration': 2.8},\n",
       " {'text': \"technically what I'm saying here that\",\n",
       "  'start': 667.839,\n",
       "  'duration': 3.44},\n",
       " {'text': \"this will only have one function let's\",\n",
       "  'start': 669.12,\n",
       "  'duration': 4.24},\n",
       " {'text': 'call it uh send task and this could be',\n",
       "  'start': 671.279,\n",
       "  'duration': 3.761},\n",
       " {'text': 'anything this is just a message which',\n",
       "  'start': 673.36,\n",
       "  'duration': 3.76},\n",
       " {'text': 'could be any and by any I mean obviously',\n",
       "  'start': 675.04,\n",
       "  'duration': 3.6},\n",
       " {'text': \"there's some protocols around it it\",\n",
       "  'start': 677.12,\n",
       "  'duration': 3.12},\n",
       " {'text': 'could be a string it could be a image it',\n",
       "  'start': 678.64,\n",
       "  'duration': 3.04},\n",
       " {'text': 'could be a lot of different things but',\n",
       "  'start': 680.24,\n",
       "  'duration': 2.88},\n",
       " {'text': \"that's what I'm saying right but\",\n",
       "  'start': 681.68,\n",
       "  'duration': 3.44},\n",
       " {'text': 'technically in MCP as well you could',\n",
       "  'start': 683.12,\n",
       "  'duration': 3.68},\n",
       " {'text': 'have the same kind of a function, right?',\n",
       "  'start': 685.12,\n",
       "  'duration': 3.839},\n",
       " {'text': 'You could technically just have one tool',\n",
       "  'start': 686.8,\n",
       "  'duration': 4.8},\n",
       " {'text': 'in MCP which says send task and the',\n",
       "  'start': 688.959,\n",
       "  'duration': 3.761},\n",
       " {'text': 'message could be any. It could be',\n",
       "  'start': 691.6,\n",
       "  'duration': 2.56},\n",
       " {'text': 'string, it could be binary, it could be',\n",
       "  'start': 692.72,\n",
       "  'duration': 3.84},\n",
       " {'text': 'anything. And now you have the same kind',\n",
       "  'start': 694.16,\n",
       "  'duration': 5.6},\n",
       " {'text': 'of API for both of them, right? So A2A',\n",
       "  'start': 696.56,\n",
       "  'duration': 5.6},\n",
       " {'text': 'is kind of like MCP with a single tool.',\n",
       "  'start': 699.76,\n",
       "  'duration': 4.319},\n",
       " {'text': \"That's what I'm trying to highlight. A2A\",\n",
       "  'start': 702.16,\n",
       "  'duration': 3.84},\n",
       " {'text': 'is like MCP server with a single tool',\n",
       "  'start': 704.079,\n",
       "  'duration': 3.841},\n",
       " {'text': 'where you just take a message as the',\n",
       "  'start': 706.0,\n",
       "  'duration': 3.2},\n",
       " {'text': 'input and the message could be anything.',\n",
       "  'start': 707.92,\n",
       "  'duration': 2.72},\n",
       " {'text': 'It could be natural language string. It',\n",
       "  'start': 709.2,\n",
       "  'duration': 3.12},\n",
       " {'text': 'could be structured. It could be a JSON',\n",
       "  'start': 710.64,\n",
       "  'duration': 3.6},\n",
       " {'text': 'object. It could literally be anything.',\n",
       "  'start': 712.32,\n",
       "  'duration': 5.28},\n",
       " {'text': 'However, most people will use MCP more',\n",
       "  'start': 714.24,\n",
       "  'duration': 6.96},\n",
       " {'text': 'like this like get tasks uh create task',\n",
       "  'start': 717.6,\n",
       "  'duration': 5.28},\n",
       " {'text': 'and stuff like that, right? Create task',\n",
       "  'start': 721.2,\n",
       "  'duration': 3.84},\n",
       " {'text': 'and maybe you have a task as well,',\n",
       "  'start': 722.88,\n",
       "  'duration': 3.44},\n",
       " {'text': 'whatever. You get the point, guys.',\n",
       "  'start': 725.04,\n",
       "  'duration': 4.4},\n",
       " {'text': 'Again, A2A is like an MCP server with',\n",
       "  'start': 726.32,\n",
       "  'duration': 5.519},\n",
       " {'text': 'just a single tool. All right? However,',\n",
       "  'start': 729.44,\n",
       "  'duration': 5.44},\n",
       " {'text': 'in MCP, the spec currently at least',\n",
       "  'start': 731.839,\n",
       "  'duration': 5.44},\n",
       " {'text': 'currently only has a description. So,',\n",
       "  'start': 734.88,\n",
       "  'duration': 4.16},\n",
       " {'text': 'the only way you can describe what this',\n",
       "  'start': 737.279,\n",
       "  'duration': 3.521},\n",
       " {'text': 'tool does is through a description. So',\n",
       "  'start': 739.04,\n",
       "  'duration': 3.2},\n",
       " {'text': 'you would have to write like a really',\n",
       "  'start': 740.8,\n",
       "  'duration': 4.0},\n",
       " {'text': 'huge description with a lot of examples.',\n",
       "  'start': 742.24,\n",
       "  'duration': 3.92},\n",
       " {'text': 'So it would become a very difficult',\n",
       "  'start': 744.8,\n",
       "  'duration': 2.719},\n",
       " {'text': \"process to do it. I'm saying you can do\",\n",
       "  'start': 746.16,\n",
       "  'duration': 4.0},\n",
       " {'text': \"it but it's really complex. So having a\",\n",
       "  'start': 747.519,\n",
       "  'duration': 4.32},\n",
       " {'text': 'single tool which can do everything in',\n",
       "  'start': 750.16,\n",
       "  'duration': 3.04},\n",
       " {'text': 'MCP is going to be a little bit hard',\n",
       "  'start': 751.839,\n",
       "  'duration': 2.56},\n",
       " {'text': 'because your description is going to be',\n",
       "  'start': 753.2,\n",
       "  'duration': 2.879},\n",
       " {'text': \"bonkers. It's going to be really hard.\",\n",
       "  'start': 754.399,\n",
       "  'duration': 4.081},\n",
       " {'text': 'Instead in A2A like we just spoke you',\n",
       "  'start': 756.079,\n",
       "  'duration': 4.721},\n",
       " {'text': 'have skills and skills has a first class',\n",
       "  'start': 758.48,\n",
       "  'duration': 3.919},\n",
       " {'text': 'concept of example. So this is much',\n",
       "  'start': 760.8,\n",
       "  'duration': 3.599},\n",
       " {'text': 'tidier. This is much neater. In fact you',\n",
       "  'start': 762.399,\n",
       "  'duration': 3.281},\n",
       " {'text': \"don't even talk about functions. You're\",\n",
       "  'start': 764.399,\n",
       "  'duration': 3.68},\n",
       " {'text': 'just like hey I just have one API send',\n",
       "  'start': 765.68,\n",
       "  'duration': 4.159},\n",
       " {'text': 'task. You give me whatever you want and',\n",
       "  'start': 768.079,\n",
       "  'duration': 3.121},\n",
       " {'text': 'these are some examples on what you can',\n",
       "  'start': 769.839,\n",
       "  'duration': 2.961},\n",
       " {'text': 'do with me. What you can do with me?',\n",
       "  'start': 771.2,\n",
       "  'duration': 3.84},\n",
       " {'text': \"That doesn't sound right. Cool. Sweet.\",\n",
       "  'start': 772.8,\n",
       "  'duration': 4.159},\n",
       " {'text': \"That's the capability discovery. I hope\",\n",
       "  'start': 775.04,\n",
       "  'duration': 3.359},\n",
       " {'text': \"this makes sense. There's one more thing\",\n",
       "  'start': 776.959,\n",
       "  'duration': 3.921},\n",
       " {'text': \"I want to highlight. Let's say in an MCP\",\n",
       "  'start': 778.399,\n",
       "  'duration': 3.841},\n",
       " {'text': 'server, we have something called create',\n",
       "  'start': 780.88,\n",
       "  'duration': 3.6},\n",
       " {'text': 'task and it takes a project and a task.',\n",
       "  'start': 782.24,\n",
       "  'duration': 3.76},\n",
       " {'text': \"Okay, let's say I have an MCP server\",\n",
       "  'start': 784.48,\n",
       "  'duration': 4.24},\n",
       " {'text': 'which takes two arguments. Now, what if',\n",
       "  'start': 786.0,\n",
       "  'duration': 4.639},\n",
       " {'text': \"the user's original question was like\",\n",
       "  'start': 788.72,\n",
       "  'duration': 3.6},\n",
       " {'text': 'create this task and they do not mention',\n",
       "  'start': 790.639,\n",
       "  'duration': 3.44},\n",
       " {'text': 'a project. What would happen? Think',\n",
       "  'start': 792.32,\n",
       "  'duration': 3.6},\n",
       " {'text': 'about that. What would happen if the',\n",
       "  'start': 794.079,\n",
       "  'duration': 3.76},\n",
       " {'text': 'user shows intent to call a particular',\n",
       "  'start': 795.92,\n",
       "  'duration': 3.52},\n",
       " {'text': \"tool but doesn't give you all the\",\n",
       "  'start': 797.839,\n",
       "  'duration': 4.641},\n",
       " {'text': 'inputs? In that case, your client, this',\n",
       "  'start': 799.44,\n",
       "  'duration': 4.8},\n",
       " {'text': 'guy would have to be smart enough to say',\n",
       "  'start': 802.48,\n",
       "  'duration': 4.159},\n",
       " {'text': 'that okay, to call this MCP tool, I need',\n",
       "  'start': 804.24,\n",
       "  'duration': 4.48},\n",
       " {'text': 'some more input. So, I need to ask for',\n",
       "  'start': 806.639,\n",
       "  'duration': 3.601},\n",
       " {'text': 'more input from the user. Right? The',\n",
       "  'start': 808.72,\n",
       "  'duration': 2.64},\n",
       " {'text': 'client would have to be smart saying',\n",
       "  'start': 810.24,\n",
       "  'duration': 2.32},\n",
       " {'text': 'that okay, you know what to call this',\n",
       "  'start': 811.36,\n",
       "  'duration': 3.039},\n",
       " {'text': 'MCB tool, I need two parameters. The',\n",
       "  'start': 812.56,\n",
       "  'duration': 3.68},\n",
       " {'text': \"user's question just has one parameter.\",\n",
       "  'start': 814.399,\n",
       "  'duration': 3.44},\n",
       " {'text': 'So, I have to go back to the user and',\n",
       "  'start': 816.24,\n",
       "  'duration': 3.599},\n",
       " {'text': 'say that hey, you know, user uh you need',\n",
       "  'start': 817.839,\n",
       "  'duration': 3.68},\n",
       " {'text': 'to give me some more information. So',\n",
       "  'start': 819.839,\n",
       "  'duration': 3.201},\n",
       " {'text': \"there's a lot of intelligence which has\",\n",
       "  'start': 821.519,\n",
       "  'duration': 2.961},\n",
       " {'text': 'to sit on the MCP client and you know',\n",
       "  'start': 823.04,\n",
       "  'duration': 2.799},\n",
       " {'text': \"we'll talk about intelligence soon as\",\n",
       "  'start': 824.48,\n",
       "  'duration': 2.72},\n",
       " {'text': 'intelligence is going to be a very big',\n",
       "  'start': 825.839,\n",
       "  'duration': 3.12},\n",
       " {'text': \"one. I hope you get it. The point I'm\",\n",
       "  'start': 827.2,\n",
       "  'duration': 4.079},\n",
       " {'text': \"trying to convey is in case of MCP it's\",\n",
       "  'start': 828.959,\n",
       "  'duration': 4.32},\n",
       " {'text': \"the client's responsibility to populate\",\n",
       "  'start': 831.279,\n",
       "  'duration': 4.0},\n",
       " {'text': 'all the parameters to be called. In case',\n",
       "  'start': 833.279,\n",
       "  'duration': 3.841},\n",
       " {'text': 'of A2A you can literally pass it',\n",
       "  'start': 835.279,\n",
       "  'duration': 3.281},\n",
       " {'text': 'everything. You can even give it an',\n",
       "  'start': 837.12,\n",
       "  'duration': 2.959},\n",
       " {'text': 'incomplete input. You can technically',\n",
       "  'start': 838.56,\n",
       "  'duration': 3.2},\n",
       " {'text': 'still give it an incomplete input and it',\n",
       "  'start': 840.079,\n",
       "  'duration': 3.601},\n",
       " {'text': 'would still work cuz A2A has built-in',\n",
       "  'start': 841.76,\n",
       "  'duration': 4.0},\n",
       " {'text': 'mechanism where the agent will ask you',\n",
       "  'start': 843.68,\n",
       "  'duration': 3.68},\n",
       " {'text': 'for further clarification. Right? So',\n",
       "  'start': 845.76,\n",
       "  'duration': 2.96},\n",
       " {'text': \"your client doesn't have to worry about\",\n",
       "  'start': 847.36,\n",
       "  'duration': 3.36},\n",
       " {'text': 'it so much. Okay. Important point. And',\n",
       "  'start': 848.72,\n",
       "  'duration': 3.76},\n",
       " {'text': 'the biggest point I want to highlight',\n",
       "  'start': 850.72,\n",
       "  'duration': 5.28},\n",
       " {'text': 'here is in case of A2A intelligence',\n",
       "  'start': 852.48,\n",
       "  'duration': 5.68},\n",
       " {'text': 'moves from the server to the client. So',\n",
       "  'start': 856.0,\n",
       "  'duration': 5.68},\n",
       " {'text': 'intelligence mostly on the server',\n",
       "  'start': 858.16,\n",
       "  'duration': 5.2},\n",
       " {'text': \"mostly. I'm not saying your client\",\n",
       "  'start': 861.68,\n",
       "  'duration': 3.92},\n",
       " {'text': \"cannot be intelligent. I'm saying in\",\n",
       "  'start': 863.36,\n",
       "  'duration': 5.2},\n",
       " {'text': 'most use cases like 80% of the use cases',\n",
       "  'start': 865.6,\n",
       "  'duration': 4.56},\n",
       " {'text': 'your client are not going to be as',\n",
       "  'start': 868.56,\n",
       "  'duration': 2.88},\n",
       " {'text': 'intelligent. Your servers are going to',\n",
       "  'start': 870.16,\n",
       "  'duration': 2.64},\n",
       " {'text': 'be more intelligent. So your client will',\n",
       "  'start': 871.44,\n",
       "  'duration': 3.12},\n",
       " {'text': 'just be like hey the user asked me this',\n",
       "  'start': 872.8,\n",
       "  'duration': 3.12},\n",
       " {'text': 'question. Can you give me an answer to',\n",
       "  'start': 874.56,\n",
       "  'duration': 2.719},\n",
       " {'text': 'this question? So the client is just',\n",
       "  'start': 875.92,\n",
       "  'duration': 2.96},\n",
       " {'text': 'like passing things around. The server',\n",
       "  'start': 877.279,\n",
       "  'duration': 2.8},\n",
       " {'text': 'will be like wait I need more', 'start': 878.88, 'duration': 2.72},\n",
       " {'text': 'clarification. The client will be like',\n",
       "  'start': 880.079,\n",
       "  'duration': 3.12},\n",
       " {'text': \"oh you need more clarification I'll just\",\n",
       "  'start': 881.6,\n",
       "  'duration': 3.2},\n",
       " {'text': 'ask the user for more clarification. So',\n",
       "  'start': 883.199,\n",
       "  'duration': 3.121},\n",
       " {'text': 'the client is just like the middleman',\n",
       "  'start': 884.8,\n",
       "  'duration': 2.64},\n",
       " {'text': \"you know it doesn't have a lot of\",\n",
       "  'start': 886.32,\n",
       "  'duration': 2.72},\n",
       " {'text': \"intelligence. I'm not saying it cannot\",\n",
       "  'start': 887.44,\n",
       "  'duration': 3.199},\n",
       " {'text': 'it can. There could be use cases which',\n",
       "  'start': 889.04,\n",
       "  'duration': 4.4},\n",
       " {'text': 'demand it but mostly the intelligence',\n",
       "  'start': 890.639,\n",
       "  'duration': 5.121},\n",
       " {'text': 'will be on the server. For MCP mostly',\n",
       "  'start': 893.44,\n",
       "  'duration': 4.079},\n",
       " {'text': 'the intelligence will be on the client.',\n",
       "  'start': 895.76,\n",
       "  'duration': 3.199},\n",
       " {'text': 'Your client will have to figure out',\n",
       "  'start': 897.519,\n",
       "  'duration': 3.12},\n",
       " {'text': 'whether there are some parameters',\n",
       "  'start': 898.959,\n",
       "  'duration': 3.361},\n",
       " {'text': 'missing or not. The client will have to',\n",
       "  'start': 900.639,\n",
       "  'duration': 2.961},\n",
       " {'text': 'figure out you know what we need some',\n",
       "  'start': 902.32,\n",
       "  'duration': 2.56},\n",
       " {'text': 'more input. Your client will have to',\n",
       "  'start': 903.6,\n",
       "  'duration': 3.2},\n",
       " {'text': 'even plan what tools to call in what',\n",
       "  'start': 904.88,\n",
       "  'duration': 4.16},\n",
       " {'text': 'format. So the client is doing a lot',\n",
       "  'start': 906.8,\n",
       "  'duration': 4.719},\n",
       " {'text': 'more work than a A2A client, right? So',\n",
       "  'start': 909.04,\n",
       "  'duration': 4.159},\n",
       " {'text': 'in MCP you need a smarter client. So',\n",
       "  'start': 911.519,\n",
       "  'duration': 3.44},\n",
       " {'text': 'intelligent is mostly on the client. The',\n",
       "  'start': 913.199,\n",
       "  'duration': 3.361},\n",
       " {'text': 'servers are dumb. The servers are just',\n",
       "  'start': 914.959,\n",
       "  'duration': 2.88},\n",
       " {'text': 'like, \"Yeah, I\\'ll do a database call.',\n",
       "  'start': 916.56,\n",
       "  'duration': 2.48},\n",
       " {'text': 'Yeah, I\\'ll do a create task.\" But for',\n",
       "  'start': 917.839,\n",
       "  'duration': 3.44},\n",
       " {'text': \"A2A, it's the other way around. However,\",\n",
       "  'start': 919.04,\n",
       "  'duration': 3.84},\n",
       " {'text': \"again, highlight this. I know I've said\",\n",
       "  'start': 921.279,\n",
       "  'duration': 4.56},\n",
       " {'text': 'this like 10 times already. But for MCP,',\n",
       "  'start': 922.88,\n",
       "  'duration': 5.199},\n",
       " {'text': 'you could have A2A like semantics and',\n",
       "  'start': 925.839,\n",
       "  'duration': 4.401},\n",
       " {'text': 'MCP as well. Like I said already, you',\n",
       "  'start': 928.079,\n",
       "  'duration': 3.841},\n",
       " {'text': 'could have just one tool which says send',\n",
       "  'start': 930.24,\n",
       "  'duration': 3.039},\n",
       " {'text': 'task. You could have a tool like this',\n",
       "  'start': 931.92,\n",
       "  'duration': 2.719},\n",
       " {'text': 'over here as well, you know, which is',\n",
       "  'start': 933.279,\n",
       "  'duration': 3.281},\n",
       " {'text': 'smart. But again, then you fall short.',\n",
       "  'start': 934.639,\n",
       "  'duration': 3.44},\n",
       " {'text': \"Like what if the input wasn't clear?\",\n",
       "  'start': 936.56,\n",
       "  'duration': 2.959},\n",
       " {'text': \"You'll have a function which gives you\",\n",
       "  'start': 938.079,\n",
       "  'duration': 2.961},\n",
       " {'text': 'back a response saying that actually I',\n",
       "  'start': 939.519,\n",
       "  'duration': 3.601},\n",
       " {'text': 'need more input. Right? So the function',\n",
       "  'start': 941.04,\n",
       "  'duration': 3.599},\n",
       " {'text': 'response for this in that case would be',\n",
       "  'start': 943.12,\n",
       "  'duration': 2.8},\n",
       " {'text': 'wait a minute you did not give me the',\n",
       "  'start': 944.639,\n",
       "  'duration': 2.801},\n",
       " {'text': 'project ID. Give me a project ID as well',\n",
       "  'start': 945.92,\n",
       "  'duration': 3.44},\n",
       " {'text': 'or give me the project name as well. But',\n",
       "  'start': 947.44,\n",
       "  'duration': 3.36},\n",
       " {'text': 'now the client would have to be smart',\n",
       "  'start': 949.36,\n",
       "  'duration': 3.279},\n",
       " {'text': 'enough to say wait the answer I got back',\n",
       "  'start': 950.8,\n",
       "  'duration': 4.159},\n",
       " {'text': \"is not the final answer. It's actually a\",\n",
       "  'start': 952.639,\n",
       "  'duration': 4.241},\n",
       " {'text': 'prompt for asking something more. So',\n",
       "  'start': 954.959,\n",
       "  'duration': 3.281},\n",
       " {'text': 'again even if you try to move', 'start': 956.88, 'duration': 3.36},\n",
       " {'text': 'intelligence to the MCP server your',\n",
       "  'start': 958.24,\n",
       "  'duration': 3.2},\n",
       " {'text': 'client will still have to be smart',\n",
       "  'start': 960.24,\n",
       "  'duration': 2.56},\n",
       " {'text': 'enough to understand that hey you know',\n",
       "  'start': 961.44,\n",
       "  'duration': 3.199},\n",
       " {'text': \"what the response I'm getting back is\",\n",
       "  'start': 962.8,\n",
       "  'duration': 3.52},\n",
       " {'text': \"actually not the answer. It's a\",\n",
       "  'start': 964.639,\n",
       "  'duration': 5.041},\n",
       " {'text': 'follow-up question. So, you can make A2A',\n",
       "  'start': 966.32,\n",
       "  'duration': 5.04},\n",
       " {'text': \"using MCP. I'm just saying it's really\",\n",
       "  'start': 969.68,\n",
       "  'duration': 4.44},\n",
       " {'text': 'convoluted to do that. Does that make',\n",
       "  'start': 971.36,\n",
       "  'duration': 5.12},\n",
       " {'text': 'sense? Explorer brings a point that what',\n",
       "  'start': 974.12,\n",
       "  'duration': 4.12},\n",
       " {'text': 'if I have an MCP server to do research.',\n",
       "  'start': 976.48,\n",
       "  'duration': 3.44},\n",
       " {'text': 'So, it essentially takes a research. It',\n",
       "  'start': 978.24,\n",
       "  'duration': 3.2},\n",
       " {'text': 'takes a user prompt. It does research',\n",
       "  'start': 979.92,\n",
       "  'duration': 3.839},\n",
       " {'text': 'and gives back an answer. So, Explorer',\n",
       "  'start': 981.44,\n",
       "  'duration': 4.0},\n",
       " {'text': 'is arguing that the server needs to be',\n",
       "  'start': 983.759,\n",
       "  'duration': 4.08},\n",
       " {'text': 'intelligent in that case. However, I',\n",
       "  'start': 985.44,\n",
       "  'duration': 4.639},\n",
       " {'text': \"would argue let's take a real world.\",\n",
       "  'start': 987.839,\n",
       "  'duration': 3.92},\n",
       " {'text': \"Let's try to extrapolate it at you know\",\n",
       "  'start': 990.079,\n",
       "  'duration': 3.361},\n",
       " {'text': \"add some more features to it. Let's say\",\n",
       "  'start': 991.759,\n",
       "  'duration': 3.681},\n",
       " {'text': 'your researcher agent first plans,',\n",
       "  'start': 993.44,\n",
       "  'duration': 3.92},\n",
       " {'text': 'confirms the plan from the end user and',\n",
       "  'start': 995.44,\n",
       "  'duration': 3.68},\n",
       " {'text': 'then does the research, right? I think',\n",
       "  'start': 997.36,\n",
       "  'duration': 3.52},\n",
       " {'text': \"that's how Google Gemini does its\",\n",
       "  'start': 999.12,\n",
       "  'duration': 3.12},\n",
       " {'text': 'research and I really love it cuz it',\n",
       "  'start': 1000.88,\n",
       "  'duration': 3.04},\n",
       " {'text': 'shows me the plan and I can iterate on',\n",
       "  'start': 1002.24,\n",
       "  'duration': 3.039},\n",
       " {'text': 'the plan and then actually do the',\n",
       "  'start': 1003.92,\n",
       "  'duration': 2.719},\n",
       " {'text': \"research. It makes I like it. Let's say\",\n",
       "  'start': 1005.279,\n",
       "  'duration': 2.641},\n",
       " {'text': 'you want to add that feature, right? In',\n",
       "  'start': 1006.639,\n",
       "  'duration': 2.88},\n",
       " {'text': 'that case, you would have to create two',\n",
       "  'start': 1007.92,\n",
       "  'duration': 3.44},\n",
       " {'text': 'tools. You would have to say, okay,',\n",
       "  'start': 1009.519,\n",
       "  'duration': 4.24},\n",
       " {'text': 'first I want a research plan, then you',\n",
       "  'start': 1011.36,\n",
       "  'duration': 4.32},\n",
       " {'text': 'would have to say research. But your',\n",
       "  'start': 1013.759,\n",
       "  'duration': 4.0},\n",
       " {'text': 'research plan will also have to be smart',\n",
       "  'start': 1015.68,\n",
       "  'duration': 3.92},\n",
       " {'text': 'because what happens if you give back a',\n",
       "  'start': 1017.759,\n",
       "  'duration': 3.281},\n",
       " {'text': 'plan, but the user is like I want to',\n",
       "  'start': 1019.6,\n",
       "  'duration': 3.599},\n",
       " {'text': 'iterate on the plan. So your plan will',\n",
       "  'start': 1021.04,\n",
       "  'duration': 4.08},\n",
       " {'text': 'be something like this I guess right',\n",
       "  'start': 1023.199,\n",
       "  'duration': 4.401},\n",
       " {'text': 'that okay what was the past messages',\n",
       "  'start': 1025.12,\n",
       "  'duration': 3.76},\n",
       " {'text': 'like a message array or something like',\n",
       "  'start': 1027.6,\n",
       "  'duration': 3.12},\n",
       " {'text': 'that that okay this was the initial plan',\n",
       "  'start': 1028.88,\n",
       "  'duration': 3.199},\n",
       " {'text': \"then the user's asking a follow-up\",\n",
       "  'start': 1030.72,\n",
       "  'duration': 3.839},\n",
       " {'text': 'question so I mean you can do it you can',\n",
       "  'start': 1032.079,\n",
       "  'duration': 4.081},\n",
       " {'text': \"model all of that conversation it's just\",\n",
       "  'start': 1034.559,\n",
       "  'duration': 3.601},\n",
       " {'text': 'so convoluted and then obviously once',\n",
       "  'start': 1036.16,\n",
       "  'duration': 3.36},\n",
       " {'text': 'your plan is done then you call the',\n",
       "  'start': 1038.16,\n",
       "  'duration': 3.44},\n",
       " {'text': \"research right so I'm saying you can do\",\n",
       "  'start': 1039.52,\n",
       "  'duration': 3.76},\n",
       " {'text': \"it with MCP I'm just saying that there's\",\n",
       "  'start': 1041.6,\n",
       "  'duration': 3.12},\n",
       " {'text': 'a lot of back and forth happening and',\n",
       "  'start': 1043.28,\n",
       "  'duration': 2.799},\n",
       " {'text': \"there's a lot of modeling which will\",\n",
       "  'start': 1044.72,\n",
       "  'duration': 2.8},\n",
       " {'text': \"have to happen and you'll have to create\",\n",
       "  'start': 1046.079,\n",
       "  'duration': 4.081},\n",
       " {'text': 'these layers right which is annoying I',\n",
       "  'start': 1047.52,\n",
       "  'duration': 4.88},\n",
       " {'text': \"am arguing something I'm arguing arguing\",\n",
       "  'start': 1050.16,\n",
       "  'duration': 4.48},\n",
       " {'text': \"don't do that okay just have a get tasks\",\n",
       "  'start': 1052.4,\n",
       "  'duration': 4.48},\n",
       " {'text': 'in your example I would do this I would',\n",
       "  'start': 1054.64,\n",
       "  'duration': 4.399},\n",
       " {'text': 'have MCP server and what would this MCP',\n",
       "  'start': 1056.88,\n",
       "  'duration': 4.08},\n",
       " {'text': 'server do this MCP server would have',\n",
       "  'start': 1059.039,\n",
       "  'duration': 4.241},\n",
       " {'text': 'some tools Google search knowledgebased',\n",
       "  'start': 1060.96,\n",
       "  'duration': 3.839},\n",
       " {'text': \"search that's it that's what my MCP\",\n",
       "  'start': 1063.28,\n",
       "  'duration': 3.6},\n",
       " {'text': 'server would do I would have an A2A',\n",
       "  'start': 1064.799,\n",
       "  'duration': 4.401},\n",
       " {'text': 'server saying I have a skill cuz',\n",
       "  'start': 1066.88,\n",
       "  'duration': 3.76},\n",
       " {'text': \"remember we have skills here so I'm just\",\n",
       "  'start': 1069.2,\n",
       "  'duration': 4.0},\n",
       " {'text': 'writing it out so we have a skill uh MCP',\n",
       "  'start': 1070.64,\n",
       "  'duration': 6.399},\n",
       " {'text': 'server tools and A2A server skills I',\n",
       "  'start': 1073.2,\n",
       "  'duration': 5.52},\n",
       " {'text': 'would say I would have an A2A server',\n",
       "  'start': 1077.039,\n",
       "  'duration': 4.561},\n",
       " {'text': 'with skills and I would say my skill is',\n",
       "  'start': 1078.72,\n",
       "  'duration': 6.4},\n",
       " {'text': \"what I do research and I've got some\",\n",
       "  'start': 1081.6,\n",
       "  'duration': 8.16},\n",
       " {'text': 'examples find me research papers on',\n",
       "  'start': 1085.12,\n",
       "  'duration': 6.64},\n",
       " {'text': 'something from Google right you could',\n",
       "  'start': 1089.76,\n",
       "  'duration': 3.6},\n",
       " {'text': 'have another example saying again',\n",
       "  'start': 1091.76,\n",
       "  'duration': 3.36},\n",
       " {'text': 'remember examples is a part of the',\n",
       "  'start': 1093.36,\n",
       "  'duration': 3.52},\n",
       " {'text': \"official spec okay I'm not making things\",\n",
       "  'start': 1095.12,\n",
       "  'duration': 3.2},\n",
       " {'text': 'up this is part of the spec I could have',\n",
       "  'start': 1096.88,\n",
       "  'duration': 4.56},\n",
       " {'text': 'another example answer this question',\n",
       "  'start': 1098.32,\n",
       "  'duration': 6.32},\n",
       " {'text': 'from my company knowledge base you see',\n",
       "  'start': 1101.44,\n",
       "  'duration': 5.2},\n",
       " {'text': \"the difference here so over here I'm\",\n",
       "  'start': 1104.64,\n",
       "  'duration': 4.08},\n",
       " {'text': 'saying hey this is a skill I do research',\n",
       "  'start': 1106.64,\n",
       "  'duration': 3.68},\n",
       " {'text': 'I have these sample things I can search',\n",
       "  'start': 1108.72,\n",
       "  'duration': 2.72},\n",
       " {'text': 'from Google I can search from',\n",
       "  'start': 1110.32,\n",
       "  'duration': 2.479},\n",
       " {'text': 'knowledgebased and this server',\n",
       "  'start': 1111.44,\n",
       "  'duration': 3.359},\n",
       " {'text': 'internally would be using MCP tools to',\n",
       "  'start': 1112.799,\n",
       "  'duration': 3.281},\n",
       " {'text': 'do the Google search and knowledgebased',\n",
       "  'start': 1114.799,\n",
       "  'duration': 2.961},\n",
       " {'text': \"search. Again, I'm saying this server\",\n",
       "  'start': 1116.08,\n",
       "  'duration': 4.4},\n",
       " {'text': 'could be an MCP server, but A2A just',\n",
       "  'start': 1117.76,\n",
       "  'duration': 4.56},\n",
       " {'text': 'seems like a better way to do it cuz A2A',\n",
       "  'start': 1120.48,\n",
       "  'duration': 3.12},\n",
       " {'text': \"has, you know what, we're getting into\",\n",
       "  'start': 1122.32,\n",
       "  'duration': 3.44},\n",
       " {'text': 'the comparison ahead. My job is to prove',\n",
       "  'start': 1123.6,\n",
       "  'duration': 4.079},\n",
       " {'text': 'why A2A is a better way to do this. So,',\n",
       "  'start': 1125.76,\n",
       "  'duration': 3.44},\n",
       " {'text': 'instead of having this research as a',\n",
       "  'start': 1127.679,\n",
       "  'duration': 3.681},\n",
       " {'text': \"tool and an MCP server, I'm going to\",\n",
       "  'start': 1129.2,\n",
       "  'duration': 4.56},\n",
       " {'text': 'argue why the A2A server needs to be a',\n",
       "  'start': 1131.36,\n",
       "  'duration': 4.16},\n",
       " {'text': 'skill. So, the research thing should be',\n",
       "  'start': 1133.76,\n",
       "  'duration': 3.84},\n",
       " {'text': 'an A2A server and why these things',\n",
       "  'start': 1135.52,\n",
       "  'duration': 3.68},\n",
       " {'text': \"should be MCP tools, right? That's the\",\n",
       "  'start': 1137.6,\n",
       "  'duration': 2.88},\n",
       " {'text': \"argument I'm going to make. But still\",\n",
       "  'start': 1139.2,\n",
       "  'duration': 3.04},\n",
       " {'text': 'the point is that an MCP server can be',\n",
       "  'start': 1140.48,\n",
       "  'duration': 3.6},\n",
       " {'text': 'smart too. Client mostly discover and',\n",
       "  'start': 1142.24,\n",
       "  'duration': 3.04},\n",
       " {'text': 'then you have a router on the client.',\n",
       "  'start': 1144.08,\n",
       "  'duration': 2.4},\n",
       " {'text': \"Yeah, that's right. You could have smart\",\n",
       "  'start': 1145.28,\n",
       "  'duration': 2.72},\n",
       " {'text': \"MCP servers too. That's why I use the\",\n",
       "  'start': 1146.48,\n",
       "  'duration': 3.92},\n",
       " {'text': 'words mostly. So intelligence is mostly',\n",
       "  'start': 1148.0,\n",
       "  'duration': 4.24},\n",
       " {'text': \"on the client. That's what I recommend.\",\n",
       "  'start': 1150.4,\n",
       "  'duration': 3.44},\n",
       " {'text': 'Uh but yeah, you could have some edge',\n",
       "  'start': 1152.24,\n",
       "  'duration': 3.2},\n",
       " {'text': 'cases where your MCP server really needs',\n",
       "  'start': 1153.84,\n",
       "  'duration': 3.04},\n",
       " {'text': 'to be smart as well. Why not? I mean you',\n",
       "  'start': 1155.44,\n",
       "  'duration': 2.72},\n",
       " {'text': 'want to do it, you can do it, right? If',\n",
       "  'start': 1156.88,\n",
       "  'duration': 3.36},\n",
       " {'text': \"you don't make MCP servers which are\",\n",
       "  'start': 1158.16,\n",
       "  'duration': 4.879},\n",
       " {'text': 'smart, A2A will eat MCP servers for',\n",
       "  'start': 1160.24,\n",
       "  'duration': 4.559},\n",
       " {'text': \"lunch with the A2A protocol. That's\",\n",
       "  'start': 1163.039,\n",
       "  'duration': 3.681},\n",
       " {'text': \"precisely what I'm going to argue. I'm\",\n",
       "  'start': 1164.799,\n",
       "  'duration': 4.481},\n",
       " {'text': 'going to argue that any MCP server which',\n",
       "  'start': 1166.72,\n",
       "  'duration': 4.959},\n",
       " {'text': 'had intelligence which was using an LLM',\n",
       "  'start': 1169.28,\n",
       "  'duration': 5.68},\n",
       " {'text': 'should and will be replaced by an A2A',\n",
       "  'start': 1171.679,\n",
       "  'duration': 4.961},\n",
       " {'text': \"server. That's my hot take and that's\",\n",
       "  'start': 1174.96,\n",
       "  'duration': 2.56},\n",
       " {'text': \"what I'm going to prove in this\",\n",
       "  'start': 1176.64,\n",
       "  'duration': 2.72},\n",
       " {'text': \"comparison. It's opinionated but hey\",\n",
       "  'start': 1177.52,\n",
       "  'duration': 3.36},\n",
       " {'text': \"that's my thing. Okay. What's the point\",\n",
       "  'start': 1179.36,\n",
       "  'duration': 3.76},\n",
       " {'text': 'of MCP server if you could make A2A',\n",
       "  'start': 1180.88,\n",
       "  'duration': 4.88},\n",
       " {'text': 'server with similar task because A2A',\n",
       "  'start': 1183.12,\n",
       "  'duration': 4.48},\n",
       " {'text': \"doesn't have the schema approach. There\",\n",
       "  'start': 1185.76,\n",
       "  'duration': 4.24},\n",
       " {'text': 'is no way in A2A to say that okay you',\n",
       "  'start': 1187.6,\n",
       "  'duration': 4.4},\n",
       " {'text': 'know what I need an input with these',\n",
       "  'start': 1190.0,\n",
       "  'duration': 3.76},\n",
       " {'text': 'precise things. So if you want to do a',\n",
       "  'start': 1192.0,\n",
       "  'duration': 3.039},\n",
       " {'text': 'Google search or something like that,',\n",
       "  'start': 1193.76,\n",
       "  'duration': 3.68},\n",
       " {'text': 'you know these parameters, A2A has no',\n",
       "  'start': 1195.039,\n",
       "  'duration': 4.0},\n",
       " {'text': 'way to describe the schema for those',\n",
       "  'start': 1197.44,\n",
       "  'duration': 3.92},\n",
       " {'text': 'parameters. A2A could add that in their',\n",
       "  'start': 1199.039,\n",
       "  'duration': 4.081},\n",
       " {'text': 'specification. Technically speaking,',\n",
       "  'start': 1201.36,\n",
       "  'duration': 3.6},\n",
       " {'text': 'they could add capability called tools',\n",
       "  'start': 1203.12,\n",
       "  'duration': 4.4},\n",
       " {'text': \"in their specs, but they haven't. Maybe\",\n",
       "  'start': 1204.96,\n",
       "  'duration': 5.2},\n",
       " {'text': \"it's to cozy up with MCP or not. I don't\",\n",
       "  'start': 1207.52,\n",
       "  'duration': 5.76},\n",
       " {'text': 'know. Currently A2A has no way to give',\n",
       "  'start': 1210.16,\n",
       "  'duration': 5.759},\n",
       " {'text': 'structured to define schemas for inputs.',\n",
       "  'start': 1213.28,\n",
       "  'duration': 4.24},\n",
       " {'text': 'Which means if you want to call a',\n",
       "  'start': 1215.919,\n",
       "  'duration': 3.601},\n",
       " {'text': 'database, if you want to like you know',\n",
       "  'start': 1217.52,\n",
       "  'duration': 5.84},\n",
       " {'text': 'do any kind of structured tool call, A2A',\n",
       "  'start': 1219.52,\n",
       "  'duration': 5.68},\n",
       " {'text': 'literally has no way to do it. There is',\n",
       "  'start': 1223.36,\n",
       "  'duration': 4.24},\n",
       " {'text': 'no way to enforce a schema in the A2A',\n",
       "  'start': 1225.2,\n",
       "  'duration': 4.719},\n",
       " {'text': 'protocol. So for tools, unfortunately,',\n",
       "  'start': 1227.6,\n",
       "  'duration': 4.559},\n",
       " {'text': 'you just have to use MCP because A2A',\n",
       "  'start': 1229.919,\n",
       "  'duration': 4.161},\n",
       " {'text': 'chose not to implement it. It could, it',\n",
       "  'start': 1232.159,\n",
       "  'duration': 4.801},\n",
       " {'text': 'chose not to do it, right? So you still',\n",
       "  'start': 1234.08,\n",
       "  'duration': 4.479},\n",
       " {'text': 'need MCP for that reason. But okay,',\n",
       "  'start': 1236.96,\n",
       "  'duration': 2.959},\n",
       " {'text': \"let's keep continuing. Let's keep\",\n",
       "  'start': 1238.559,\n",
       "  'duration': 3.201},\n",
       " {'text': \"continuing. We've already given the end\",\n",
       "  'start': 1239.919,\n",
       "  'duration': 3.76},\n",
       " {'text': 'answer at this point. So now the stream',\n",
       "  'start': 1241.76,\n",
       "  'duration': 3.6},\n",
       " {'text': 'has become like instead of arriving to',\n",
       "  'start': 1243.679,\n",
       "  'duration': 3.041},\n",
       " {'text': \"this conclusion, we're going to prove\",\n",
       "  'start': 1245.36,\n",
       "  'duration': 2.64},\n",
       " {'text': 'why is this conclusion a better',\n",
       "  'start': 1246.72,\n",
       "  'duration': 3.439},\n",
       " {'text': 'conclusion? Oh my god, we just flipped',\n",
       "  'start': 1248.0,\n",
       "  'duration': 3.919},\n",
       " {'text': 'the stream over. You guys always do that',\n",
       "  'start': 1250.159,\n",
       "  'duration': 4.161},\n",
       " {'text': 'to me. Like my flow just goes for a',\n",
       "  'start': 1251.919,\n",
       "  'duration': 5.921},\n",
       " {'text': 'toss. I mean, okay,', 'start': 1254.32, 'duration': 3.52},\n",
       " {'text': 'cool. Yeah. So the next point I want to',\n",
       "  'start': 1257.88,\n",
       "  'duration': 5.96},\n",
       " {'text': 'say is the task scope. So again, just',\n",
       "  'start': 1261.52,\n",
       "  'duration': 4.0},\n",
       " {'text': \"covering what we already discussed. I'm\",\n",
       "  'start': 1263.84,\n",
       "  'duration': 3.04},\n",
       " {'text': 'going to actually put it up here. We',\n",
       "  'start': 1265.52,\n",
       "  'duration': 2.8},\n",
       " {'text': 'already discussed this because the',\n",
       "  'start': 1266.88,\n",
       "  'duration': 3.84},\n",
       " {'text': 'skills are so broad. The scope of A2A is',\n",
       "  'start': 1268.32,\n",
       "  'duration': 3.76},\n",
       " {'text': \"not well defined. Right? You're just\",\n",
       "  'start': 1270.72,\n",
       "  'duration': 2.88},\n",
       " {'text': \"giving examples. So you're not really\",\n",
       "  'start': 1272.08,\n",
       "  'duration': 2.719},\n",
       " {'text': \"restricting the scope. You're just like,\",\n",
       "  'start': 1273.6,\n",
       "  'duration': 3.52},\n",
       " {'text': '\"Hey, a few examples. Stick with it.\"',\n",
       "  'start': 1274.799,\n",
       "  'duration': 4.721},\n",
       " {'text': \"But there's nothing forcing your client\",\n",
       "  'start': 1277.12,\n",
       "  'duration': 4.4},\n",
       " {'text': 'to stick with it, right? In this notion',\n",
       "  'start': 1279.52,\n",
       "  'duration': 3.2},\n",
       " {'text': 'server, the client could technically',\n",
       "  'start': 1281.52,\n",
       "  'duration': 3.2},\n",
       " {'text': 'say, \"Give me the scores for tonight\\'s',\n",
       "  'start': 1282.72,\n",
       "  'duration': 3.68},\n",
       " {'text': 'baseball game.\" I don\\'t know. It could',\n",
       "  'start': 1284.72,\n",
       "  'duration': 3.6},\n",
       " {'text': \"do that. There's nothing stopping it. In\",\n",
       "  'start': 1286.4,\n",
       "  'duration': 4.48},\n",
       " {'text': 'MCP, there is no tools to get results of',\n",
       "  'start': 1288.32,\n",
       "  'duration': 4.0},\n",
       " {'text': \"a baseball game. I don't know even why\",\n",
       "  'start': 1290.88,\n",
       "  'duration': 2.96},\n",
       " {'text': \"I'm saying baseball. I don't even watch\",\n",
       "  'start': 1292.32,\n",
       "  'duration': 4.64},\n",
       " {'text': 'baseball. But okay, in MCP, the scope is',\n",
       "  'start': 1293.84,\n",
       "  'duration': 4.64},\n",
       " {'text': 'restricted to get task and creating',\n",
       "  'start': 1296.96,\n",
       "  'duration': 4.0},\n",
       " {'text': 'task, right? In A2A there is no such way',\n",
       "  'start': 1298.48,\n",
       "  'duration': 4.88},\n",
       " {'text': 'to do that which means your server is',\n",
       "  'start': 1300.96,\n",
       "  'duration': 4.4},\n",
       " {'text': 'going to be complex. You need guardrails',\n",
       "  'start': 1303.36,\n",
       "  'duration': 5.04},\n",
       " {'text': \"in A2A clearly it's very well defined. I\",\n",
       "  'start': 1305.36,\n",
       "  'duration': 5.52},\n",
       " {'text': 'think one more thing goes in that in MCP',\n",
       "  'start': 1308.4,\n",
       "  'duration': 4.399},\n",
       " {'text': \"input strictness and I think you'll also\",\n",
       "  'start': 1310.88,\n",
       "  'duration': 3.76},\n",
       " {'text': 'get my thought process on like how am I',\n",
       "  'start': 1312.799,\n",
       "  'duration': 3.641},\n",
       " {'text': 'comparing these things right? So input',\n",
       "  'start': 1314.64,\n",
       "  'duration': 4.88},\n",
       " {'text': \"strictness not strict at all. It's\",\n",
       "  'start': 1316.44,\n",
       "  'duration': 5.16},\n",
       " {'text': \"multimodal. I mean it's free flow. Do\",\n",
       "  'start': 1319.52,\n",
       "  'duration': 4.399},\n",
       " {'text': 'whatever you want. Mostly strict. I also',\n",
       "  'start': 1321.6,\n",
       "  'duration': 4.079},\n",
       " {'text': 'add mostly over here because I know we',\n",
       "  'start': 1323.919,\n",
       "  'duration': 4.24},\n",
       " {'text': 'can have smart MCP servers man. I mean,',\n",
       "  'start': 1325.679,\n",
       "  'duration': 4.321},\n",
       " {'text': 'you could technically have smart MCP',\n",
       "  'start': 1328.159,\n",
       "  'duration': 3.201},\n",
       " {'text': 'servers. Why not? I mean, you could do',\n",
       "  'start': 1330.0,\n",
       "  'duration': 3.679},\n",
       " {'text': \"it, but mostly it's going to be strict,\",\n",
       "  'start': 1331.36,\n",
       "  'duration': 3.76},\n",
       " {'text': \"right? Because you're defining the\",\n",
       "  'start': 1333.679,\n",
       "  'duration': 3.201},\n",
       " {'text': 'freaking type of every parameter you',\n",
       "  'start': 1335.12,\n",
       "  'duration': 3.36},\n",
       " {'text': \"want, right? So, it's as strict as it\",\n",
       "  'start': 1336.88,\n",
       "  'duration': 3.84},\n",
       " {'text': 'gets. This is a function call. MCP is',\n",
       "  'start': 1338.48,\n",
       "  'duration': 4.72},\n",
       " {'text': \"just RPC. It's remote procedural calls.\",\n",
       "  'start': 1340.72,\n",
       "  'duration': 4.64},\n",
       " {'text': \"Okay? Now, we're going to get into the\",\n",
       "  'start': 1343.2,\n",
       "  'duration': 4.24},\n",
       " {'text': \"big stuff. So, right now, I've just been\",\n",
       "  'start': 1345.36,\n",
       "  'duration': 4.64},\n",
       " {'text': \"stating some lowhanging fruits. I'll get\",\n",
       "  'start': 1347.44,\n",
       "  'duration': 4.479},\n",
       " {'text': 'into justifying this now. So, now comes',\n",
       "  'start': 1350.0,\n",
       "  'duration': 3.679},\n",
       " {'text': 'a little a heavier portion. So, I want',\n",
       "  'start': 1351.919,\n",
       "  'duration': 5.321},\n",
       " {'text': 'to take a pause here.', 'start': 1353.679, 'duration': 3.561},\n",
       " {'text': 'Uh now I want to get into some big',\n",
       "  'start': 1357.679,\n",
       "  'duration': 5.12},\n",
       " {'text': 'differences on why A2A servers should be',\n",
       "  'start': 1359.919,\n",
       "  'duration': 4.961},\n",
       " {'text': 'intelligent. So I want to focus on this',\n",
       "  'start': 1362.799,\n",
       "  'duration': 3.841},\n",
       " {'text': 'why intelligent servers should be A2A',\n",
       "  'start': 1364.88,\n",
       "  'duration': 3.52},\n",
       " {'text': 'and why dumb servers should be MCP.',\n",
       "  'start': 1366.64,\n",
       "  'duration': 3.12},\n",
       " {'text': \"Okay, that's what we need to get at.\",\n",
       "  'start': 1368.4,\n",
       "  'duration': 3.36},\n",
       " {'text': \"Let's start with the obvious point which\",\n",
       "  'start': 1369.76,\n",
       "  'duration': 3.12},\n",
       " {'text': \"we've already covered. I'll just\",\n",
       "  'start': 1371.76,\n",
       "  'duration': 2.799},\n",
       " {'text': 'reiterate it. If you have an MCP for',\n",
       "  'start': 1372.88,\n",
       "  'duration': 3.52},\n",
       " {'text': 'notion, right, which does create get',\n",
       "  'start': 1374.559,\n",
       "  'duration': 3.681},\n",
       " {'text': 'task and create task or whatever,',\n",
       "  'start': 1376.4,\n",
       "  'duration': 3.519},\n",
       " {'text': \"somebody has to make a plan. Let's take\",\n",
       "  'start': 1378.24,\n",
       "  'duration': 3.6},\n",
       " {'text': 'a simpler example which I think a lot of',\n",
       "  'start': 1379.919,\n",
       "  'duration': 4.081},\n",
       " {'text': \"people have seen out in the world. Let's\",\n",
       "  'start': 1381.84,\n",
       "  'duration': 3.76},\n",
       " {'text': 'say you have an MCP server for a',\n",
       "  'start': 1384.0,\n",
       "  'duration': 4.159},\n",
       " {'text': \"database cuz who isn't doing this,\",\n",
       "  'start': 1385.6,\n",
       "  'duration': 4.959},\n",
       " {'text': \"right? Nobody's doing this. Nobody's\",\n",
       "  'start': 1388.159,\n",
       "  'duration': 4.321},\n",
       " {'text': 'building MCP servers for databases,',\n",
       "  'start': 1390.559,\n",
       "  'duration': 3.521},\n",
       " {'text': \"right? I mean, that's absurd. Who would\",\n",
       "  'start': 1392.48,\n",
       "  'duration': 4.559},\n",
       " {'text': \"do that? I'm kidding. So, yeah. So,\",\n",
       "  'start': 1394.08,\n",
       "  'duration': 4.959},\n",
       " {'text': \"let's say we have a database. Okay. And\",\n",
       "  'start': 1397.039,\n",
       "  'duration': 4.561},\n",
       " {'text': 'your database has a couple of tools. So,',\n",
       "  'start': 1399.039,\n",
       "  'duration': 4.561},\n",
       " {'text': 'it has a tool called get schema and it',\n",
       "  'start': 1401.6,\n",
       "  'duration': 4.0},\n",
       " {'text': 'has a tool called run query, right? So,',\n",
       "  'start': 1403.6,\n",
       "  'duration': 3.76},\n",
       " {'text': 'get schema gets you the schema of the',\n",
       "  'start': 1405.6,\n",
       "  'duration': 3.52},\n",
       " {'text': 'database. Run query takes a SQL query',\n",
       "  'start': 1407.36,\n",
       "  'duration': 4.16},\n",
       " {'text': 'and runs it. the MCP client, the client',\n",
       "  'start': 1409.12,\n",
       "  'duration': 4.32},\n",
       " {'text': 'which is sitting here would have to be',\n",
       "  'start': 1411.52,\n",
       "  'duration': 4.159},\n",
       " {'text': 'smart enough to first decide that you',\n",
       "  'start': 1413.44,\n",
       "  'duration': 3.84},\n",
       " {'text': 'know what first I need to get the schema',\n",
       "  'start': 1415.679,\n",
       "  'duration': 3.041},\n",
       " {'text': 'based on the schema I need to generate',\n",
       "  'start': 1417.28,\n",
       "  'duration': 2.72},\n",
       " {'text': 'the query and then run the query',\n",
       "  'start': 1418.72,\n",
       "  'duration': 2.72},\n",
       " {'text': 'something like that right no matter what',\n",
       "  'start': 1420.0,\n",
       "  'duration': 2.88},\n",
       " {'text': \"your MCP server is you'll always need\",\n",
       "  'start': 1421.44,\n",
       "  'duration': 3.92},\n",
       " {'text': 'some kind of intelligence now like also',\n",
       "  'start': 1422.88,\n",
       "  'duration': 4.0},\n",
       " {'text': 'we discussed you could have something',\n",
       "  'start': 1425.36,\n",
       "  'duration': 3.6},\n",
       " {'text': 'like this you could have an MCP tool',\n",
       "  'start': 1426.88,\n",
       "  'duration': 3.52},\n",
       " {'text': 'which is essentially run natural',\n",
       "  'start': 1428.96,\n",
       "  'duration': 3.839},\n",
       " {'text': 'language query in this it seems like the',\n",
       "  'start': 1430.4,\n",
       "  'duration': 4.159},\n",
       " {'text': 'client is becoming dumb again right like',\n",
       "  'start': 1432.799,\n",
       "  'duration': 3.921},\n",
       " {'text': 'it can just do this correct uh it can',\n",
       "  'start': 1434.559,\n",
       "  'duration': 3.521},\n",
       " {'text': 'just take the natural language and pass',\n",
       "  'start': 1436.72,\n",
       "  'duration': 3.28},\n",
       " {'text': \"this and this works Don't get me wrong,\",\n",
       "  'start': 1438.08,\n",
       "  'duration': 4.32},\n",
       " {'text': 'this works. You could have an MCP server',\n",
       "  'start': 1440.0,\n",
       "  'duration': 4.159},\n",
       " {'text': 'which, you know, gets the schema, does',\n",
       "  'start': 1442.4,\n",
       "  'duration': 3.12},\n",
       " {'text': 'all that fun stuff and, you know, then',\n",
       "  'start': 1444.159,\n",
       "  'duration': 3.041},\n",
       " {'text': 'calls the database. So, yeah, you could',\n",
       "  'start': 1445.52,\n",
       "  'duration': 3.519},\n",
       " {'text': 'technically have an MCP server which',\n",
       "  'start': 1447.2,\n",
       "  'duration': 3.92},\n",
       " {'text': 'does this, but once you start getting',\n",
       "  'start': 1449.039,\n",
       "  'duration': 4.721},\n",
       " {'text': 'into the real world examples, right, and',\n",
       "  'start': 1451.12,\n",
       "  'duration': 4.4},\n",
       " {'text': \"you figure out something like the user's\",\n",
       "  'start': 1453.76,\n",
       "  'duration': 4.24},\n",
       " {'text': 'asking a question about tasks, but there',\n",
       "  'start': 1455.52,\n",
       "  'duration': 5.68},\n",
       " {'text': 'is no table called tasks. Now, this tool',\n",
       "  'start': 1458.0,\n",
       "  'duration': 5.36},\n",
       " {'text': 'will suddenly give you a response back',\n",
       "  'start': 1461.2,\n",
       "  'duration': 5.2},\n",
       " {'text': 'saying, \"Wait a minute. Know what table',\n",
       "  'start': 1463.36,\n",
       "  'duration': 4.799},\n",
       " {'text': \"you're talking about? Did you mean\",\n",
       "  'start': 1466.4,\n",
       "  'duration': 3.36},\n",
       " {'text': 'this?\" Right? Something like that. Wait',\n",
       "  'start': 1468.159,\n",
       "  'duration': 2.64},\n",
       " {'text': \"a minute. I don't know what you're\",\n",
       "  'start': 1469.76,\n",
       "  'duration': 3.039},\n",
       " {'text': 'talking about. Did you mean this? Or',\n",
       "  'start': 1470.799,\n",
       "  'duration': 4.401},\n",
       " {'text': 'maybe the MCP server says, \"Okay, I want',\n",
       "  'start': 1472.799,\n",
       "  'duration': 6.24},\n",
       " {'text': 'to run this query. Is that fine?\" Right?',\n",
       "  'start': 1475.2,\n",
       "  'duration': 6.719},\n",
       " {'text': \"So, or it could be like because it's a\",\n",
       "  'start': 1479.039,\n",
       "  'duration': 4.721},\n",
       " {'text': \"read query, here's the results of the\",\n",
       "  'start': 1481.919,\n",
       "  'duration': 3.36},\n",
       " {'text': 'read query. You see what just happened',\n",
       "  'start': 1483.76,\n",
       "  'duration': 4.0},\n",
       " {'text': \"here? We're breaking it down. So, either\",\n",
       "  'start': 1485.279,\n",
       "  'duration': 4.241},\n",
       " {'text': 'we could have a situation wherein if it',\n",
       "  'start': 1487.76,\n",
       "  'duration': 3.279},\n",
       " {'text': 'was a read query, we directly get the',\n",
       "  'start': 1489.52,\n",
       "  'duration': 3.279},\n",
       " {'text': 'result of the query. If it was a write',\n",
       "  'start': 1491.039,\n",
       "  'duration': 3.52},\n",
       " {'text': 'query, then we say, \"Hey, we want to run',\n",
       "  'start': 1492.799,\n",
       "  'duration': 3.601},\n",
       " {'text': 'this query. Is that fine?\" And if you',\n",
       "  'start': 1494.559,\n",
       "  'duration': 3.441},\n",
       " {'text': 'give me some very stupid query, then',\n",
       "  'start': 1496.4,\n",
       "  'duration': 2.72},\n",
       " {'text': 'I\\'ll be like, \"Hey, I don\\'t even know',\n",
       "  'start': 1498.0,\n",
       "  'duration': 2.32},\n",
       " {'text': \"what you're talking about. Are you\",\n",
       "  'start': 1499.12,\n",
       "  'duration': 2.4},\n",
       " {'text': 'talking about something like this',\n",
       "  'start': 1500.32,\n",
       "  'duration': 3.359},\n",
       " {'text': 'instead?\" How do you model this? So the',\n",
       "  'start': 1501.52,\n",
       "  'duration': 4.72},\n",
       " {'text': 'moment you get a response back, now your',\n",
       "  'start': 1503.679,\n",
       "  'duration': 4.24},\n",
       " {'text': 'client will need intelligence cuz your',\n",
       "  'start': 1506.24,\n",
       "  'duration': 3.039},\n",
       " {'text': 'client will have to say, \"Wait, the',\n",
       "  'start': 1507.919,\n",
       "  'duration': 4.081},\n",
       " {'text': 'response I got back, is that response a',\n",
       "  'start': 1509.279,\n",
       "  'duration': 5.361},\n",
       " {'text': 'final response? Is that response',\n",
       "  'start': 1512.0,\n",
       "  'duration': 5.12},\n",
       " {'text': 'something the human has to confirm? Is',\n",
       "  'start': 1514.64,\n",
       "  'duration': 4.08},\n",
       " {'text': 'that response something I need to',\n",
       "  'start': 1517.12,\n",
       "  'duration': 4.0},\n",
       " {'text': 'iterate on myself?\" How do you know? You',\n",
       "  'start': 1518.72,\n",
       "  'duration': 3.92},\n",
       " {'text': 'need intelligence just to understand',\n",
       "  'start': 1521.12,\n",
       "  'duration': 3.2},\n",
       " {'text': 'whether the task was completed or not.',\n",
       "  'start': 1522.64,\n",
       "  'duration': 3.519},\n",
       " {'text': \"And now again you'll be like but wait\",\n",
       "  'start': 1524.32,\n",
       "  'duration': 3.359},\n",
       " {'text': 'you know I can have a separate tool for',\n",
       "  'start': 1526.159,\n",
       "  'duration': 2.88},\n",
       " {'text': 'write queries a separate tool for read',\n",
       "  'start': 1527.679,\n",
       "  'duration': 3.681},\n",
       " {'text': \"queries again you'll have to decide\",\n",
       "  'start': 1529.039,\n",
       "  'duration': 3.76},\n",
       " {'text': 'which tool to call and your client',\n",
       "  'start': 1531.36,\n",
       "  'duration': 3.439},\n",
       " {'text': 'becomes intelligent. The point is',\n",
       "  'start': 1532.799,\n",
       "  'duration': 5.281},\n",
       " {'text': 'because MCP is just a request response',\n",
       "  'start': 1534.799,\n",
       "  'duration': 5.841},\n",
       " {'text': 'format the client will either have to',\n",
       "  'start': 1538.08,\n",
       "  'duration': 4.64},\n",
       " {'text': 'pay attention to what tool to call.',\n",
       "  'start': 1540.64,\n",
       "  'duration': 3.84},\n",
       " {'text': \"You'll need intelligence in figuring out\",\n",
       "  'start': 1542.72,\n",
       "  'duration': 4.559},\n",
       " {'text': 'what tool to call or you will need',\n",
       "  'start': 1544.48,\n",
       "  'duration': 5.12},\n",
       " {'text': 'intelligence on how to parse the',\n",
       "  'start': 1547.279,\n",
       "  'duration': 4.961},\n",
       " {'text': 'response. So you need intelligence for',\n",
       "  'start': 1549.6,\n",
       "  'duration': 4.559},\n",
       " {'text': 'one of these two things just because of',\n",
       "  'start': 1552.24,\n",
       "  'duration': 4.4},\n",
       " {'text': \"the nature of MCP. Again, I'm not saying\",\n",
       "  'start': 1554.159,\n",
       "  'duration': 4.241},\n",
       " {'text': 'having intelligence on the client is a',\n",
       "  'start': 1556.64,\n",
       "  'duration': 3.84},\n",
       " {'text': \"bad thing. I'm not saying that at all.\",\n",
       "  'start': 1558.4,\n",
       "  'duration': 3.759},\n",
       " {'text': \"I'm just saying that this is an\",\n",
       "  'start': 1560.48,\n",
       "  'duration': 3.12},\n",
       " {'text': \"additional step you'll have to worry\",\n",
       "  'start': 1562.159,\n",
       "  'duration': 3.601},\n",
       " {'text': \"about, right? You'll have to worry about\",\n",
       "  'start': 1563.6,\n",
       "  'duration': 3.76},\n",
       " {'text': 'figuring out which tool to call or',\n",
       "  'start': 1565.76,\n",
       "  'duration': 3.279},\n",
       " {'text': \"you'll have to worry about interpreting\",\n",
       "  'start': 1567.36,\n",
       "  'duration': 3.679},\n",
       " {'text': \"the response. It's not clear. Let's see\",\n",
       "  'start': 1569.039,\n",
       "  'duration': 3.921},\n",
       " {'text': 'how A2A does this. And A2 has a really',\n",
       "  'start': 1571.039,\n",
       "  'duration': 3.681},\n",
       " {'text': 'good example in the specification. So',\n",
       "  'start': 1572.96,\n",
       "  'duration': 3.76},\n",
       " {'text': \"I'm just going to write that and then if\",\n",
       "  'start': 1574.72,\n",
       "  'duration': 3.6},\n",
       " {'text': 'you want we can extrapolate it further',\n",
       "  'start': 1576.72,\n",
       "  'duration': 3.6},\n",
       " {'text': \"together. Let's say we have a question\",\n",
       "  'start': 1578.32,\n",
       "  'duration': 3.839},\n",
       " {'text': 'saying that okay request a new phone or',\n",
       "  'start': 1580.32,\n",
       "  'duration': 3.92},\n",
       " {'text': 'run this query whatever right multiple',\n",
       "  'start': 1582.159,\n",
       "  'duration': 4.081},\n",
       " {'text': 'things can happen. So okay wait wait',\n",
       "  'start': 1584.24,\n",
       "  'duration': 3.439},\n",
       " {'text': 'wait wait wait wait wait sorry I need to',\n",
       "  'start': 1586.24,\n",
       "  'duration': 3.039},\n",
       " {'text': 'backtrack because I know this is a new',\n",
       "  'start': 1587.679,\n",
       "  'duration': 3.281},\n",
       " {'text': 'video so we did not speak about it so',\n",
       "  'start': 1589.279,\n",
       "  'duration': 4.64},\n",
       " {'text': \"let's speak about this first in MCP you\",\n",
       "  'start': 1590.96,\n",
       "  'duration': 4.8},\n",
       " {'text': 'have something called tasks right so you',\n",
       "  'start': 1593.919,\n",
       "  'duration': 3.921},\n",
       " {'text': 'submit tasks to users again if you want',\n",
       "  'start': 1595.76,\n",
       "  'duration': 5.039},\n",
       " {'text': 'a detailed deep dive on the A2A spec',\n",
       "  'start': 1597.84,\n",
       "  'duration': 4.64},\n",
       " {'text': 'check out my previous video but',\n",
       "  'start': 1600.799,\n",
       "  'duration': 3.201},\n",
       " {'text': 'essentially you submit task and task',\n",
       "  'start': 1602.48,\n",
       "  'duration': 2.96},\n",
       " {'text': \"could be like anything it's natural\",\n",
       "  'start': 1604.0,\n",
       "  'duration': 2.559},\n",
       " {'text': 'language it could be an image it could',\n",
       "  'start': 1605.44,\n",
       "  'duration': 2.64},\n",
       " {'text': 'be anything so you essentially submit a',\n",
       "  'start': 1606.559,\n",
       "  'duration': 4.561},\n",
       " {'text': 'task right now the server might be able',\n",
       "  'start': 1608.08,\n",
       "  'duration': 5.36},\n",
       " {'text': 'to finish that task for you straight up',\n",
       "  'start': 1611.12,\n",
       "  'duration': 3.679},\n",
       " {'text': \"right if it's a read query it could\",\n",
       "  'start': 1613.44,\n",
       "  'duration': 2.64},\n",
       " {'text': 'finish it straight up or it could be',\n",
       "  'start': 1614.799,\n",
       "  'duration': 2.88},\n",
       " {'text': 'like wait I have some more input to ask',\n",
       "  'start': 1616.08,\n",
       "  'duration': 3.68},\n",
       " {'text': \"you correct so let's say in this case\",\n",
       "  'start': 1617.679,\n",
       "  'duration': 4.321},\n",
       " {'text': \"there's more information to ask so now\",\n",
       "  'start': 1619.76,\n",
       "  'duration': 4.48},\n",
       " {'text': 'what happens is the server tells you',\n",
       "  'start': 1622.0,\n",
       "  'duration': 4.08},\n",
       " {'text': 'back that hey wait a minute input',\n",
       "  'start': 1624.24,\n",
       "  'duration': 4.48},\n",
       " {'text': \"required there's an explicit message\",\n",
       "  'start': 1626.08,\n",
       "  'duration': 4.24},\n",
       " {'text': \"called input required or there's an\",\n",
       "  'start': 1628.72,\n",
       "  'duration': 3.76},\n",
       " {'text': 'explicit state called input required',\n",
       "  'start': 1630.32,\n",
       "  'duration': 5.04},\n",
       " {'text': 'with a very precise question so A2A is',\n",
       "  'start': 1632.48,\n",
       "  'duration': 5.6},\n",
       " {'text': \"explicit it's saying that I did get\",\n",
       "  'start': 1635.36,\n",
       "  'duration': 5.199},\n",
       " {'text': \"something back from my server but it's\",\n",
       "  'start': 1638.08,\n",
       "  'duration': 4.24},\n",
       " {'text': \"not a response you don't have to figure\",\n",
       "  'start': 1640.559,\n",
       "  'duration': 4.081},\n",
       " {'text': 'out whether I got the response or not it',\n",
       "  'start': 1642.32,\n",
       "  'duration': 3.92},\n",
       " {'text': 'explicitly saying, \"Hey, you know what?',\n",
       "  'start': 1644.64,\n",
       "  'duration': 3.6},\n",
       " {'text': 'I need some more input and this is where',\n",
       "  'start': 1646.24,\n",
       "  'duration': 3.679},\n",
       " {'text': 'now I could have a dumb client which',\n",
       "  'start': 1648.24,\n",
       "  'duration': 3.28},\n",
       " {'text': 'could just take this and give it to the',\n",
       "  'start': 1649.919,\n",
       "  'duration': 3.921},\n",
       " {'text': 'user directly on the UI or it could be a',\n",
       "  'start': 1651.52,\n",
       "  'duration': 4.08},\n",
       " {'text': 'smarter client which uses intelligence',\n",
       "  'start': 1653.84,\n",
       "  'duration': 3.199},\n",
       " {'text': 'to do something fun.\" Again, there\\'s',\n",
       "  'start': 1655.6,\n",
       "  'duration': 2.72},\n",
       " {'text': 'nothing wrong with intelligence on the',\n",
       "  'start': 1657.039,\n",
       "  'duration': 2.721},\n",
       " {'text': \"client. I'm just saying you don't have\",\n",
       "  'start': 1658.32,\n",
       "  'duration': 2.8},\n",
       " {'text': 'to figure out whether the task was',\n",
       "  'start': 1659.76,\n",
       "  'duration': 3.36},\n",
       " {'text': \"complete or not. So, obviously, let's\",\n",
       "  'start': 1661.12,\n",
       "  'duration': 3.76},\n",
       " {'text': 'say, you know, our client is dumb in',\n",
       "  'start': 1663.12,\n",
       "  'duration': 3.12},\n",
       " {'text': 'this case, so it just spits it on the',\n",
       "  'start': 1664.88,\n",
       "  'duration': 3.2},\n",
       " {'text': 'user and the user just types in android',\n",
       "  'start': 1666.24,\n",
       "  'duration': 4.72},\n",
       " {'text': 'on the UI, right? So, again, so you send',\n",
       "  'start': 1668.08,\n",
       "  'duration': 4.8},\n",
       " {'text': 'this question to uh to the server.',\n",
       "  'start': 1670.96,\n",
       "  'duration': 3.28},\n",
       " {'text': \"Again, you're just submitting the task\",\n",
       "  'start': 1672.88,\n",
       "  'duration': 2.799},\n",
       " {'text': \"against the same task ID. So, you're\",\n",
       "  'start': 1674.24,\n",
       "  'duration': 3.12},\n",
       " {'text': \"like you're continuing a conversation\",\n",
       "  'start': 1675.679,\n",
       "  'duration': 3.041},\n",
       " {'text': \"with the server, right? So, you're just\",\n",
       "  'start': 1677.36,\n",
       "  'duration': 2.4},\n",
       " {'text': 'saying, \"Hey, I want to continue the',\n",
       "  'start': 1678.72,\n",
       "  'duration': 2.64},\n",
       " {'text': 'conversation with the same task which we',\n",
       "  'start': 1679.76,\n",
       "  'duration': 3.279},\n",
       " {'text': 'just started with earlier.\" Uh, and I\\'m',\n",
       "  'start': 1681.36,\n",
       "  'duration': 3.199},\n",
       " {'text': \"just saying Android. Again, I'm not\",\n",
       "  'start': 1683.039,\n",
       "  'duration': 2.88},\n",
       " {'text': \"giving anything else. Remember, it's\",\n",
       "  'start': 1684.559,\n",
       "  'duration': 2.801},\n",
       " {'text': \"like a conversation. So, I don't need to\",\n",
       "  'start': 1685.919,\n",
       "  'duration': 2.721},\n",
       " {'text': \"be explicit. I'm just answering this\",\n",
       "  'start': 1687.36,\n",
       "  'duration': 3.439},\n",
       " {'text': 'question. Now, the server says state is',\n",
       "  'start': 1688.64,\n",
       "  'duration': 4.88},\n",
       " {'text': 'completed. Very explicit, extremely',\n",
       "  'start': 1690.799,\n",
       "  'duration': 5.201},\n",
       " {'text': \"explicit that, hey, I'm done. Now, I\",\n",
       "  'start': 1693.52,\n",
       "  'duration': 4.96},\n",
       " {'text': \"have an answer. We're done. Okay? And\",\n",
       "  'start': 1696.0,\n",
       "  'duration': 3.919},\n",
       " {'text': \"it's not giving us a message back.\",\n",
       "  'start': 1698.48,\n",
       "  'duration': 3.52},\n",
       " {'text': 'Remember in the earlier thing it was it',\n",
       "  'start': 1699.919,\n",
       "  'duration': 3.681},\n",
       " {'text': 'was giving us a message back. It was',\n",
       "  'start': 1702.0,\n",
       "  'duration': 3.679},\n",
       " {'text': 'saying hey this is my response to the',\n",
       "  'start': 1703.6,\n",
       "  'duration': 3.92},\n",
       " {'text': 'task. My status is input required and I',\n",
       "  'start': 1705.679,\n",
       "  'duration': 4.641},\n",
       " {'text': \"have a message for you. I'm conversing\",\n",
       "  'start': 1707.52,\n",
       "  'duration': 4.8},\n",
       " {'text': 'with you. I am participating in a',\n",
       "  'start': 1710.32,\n",
       "  'duration': 4.239},\n",
       " {'text': \"conversation or I'm collaborating with\",\n",
       "  'start': 1712.32,\n",
       "  'duration': 4.239},\n",
       " {'text': 'you to complete the task. Right? So this',\n",
       "  'start': 1714.559,\n",
       "  'duration': 5.12},\n",
       " {'text': 'is a message is a collaboration. However',\n",
       "  'start': 1716.559,\n",
       "  'duration': 4.84},\n",
       " {'text': 'once your task is done you get an',\n",
       "  'start': 1719.679,\n",
       "  'duration': 4.48},\n",
       " {'text': 'artifact that hey order confirmation. So',\n",
       "  'start': 1721.399,\n",
       "  'duration': 3.961},\n",
       " {'text': 'we have an artifact called order',\n",
       "  'start': 1724.159,\n",
       "  'duration': 2.561},\n",
       " {'text': 'confirmation and you can have some',\n",
       "  'start': 1725.36,\n",
       "  'duration': 2.799},\n",
       " {'text': 'details. If you were generating an',\n",
       "  'start': 1726.72,\n",
       "  'duration': 3.199},\n",
       " {'text': 'image, the artifact would be the image.',\n",
       "  'start': 1728.159,\n",
       "  'duration': 4.161},\n",
       " {'text': 'If you were creating a task, the task ID',\n",
       "  'start': 1729.919,\n",
       "  'duration': 4.561},\n",
       " {'text': 'could be the artifact. The point is once',\n",
       "  'start': 1732.32,\n",
       "  'duration': 5.359},\n",
       " {'text': 'you get the artifact, you know for sure',\n",
       "  'start': 1734.48,\n",
       "  'duration': 5.84},\n",
       " {'text': 'that hey, things are done. I have an',\n",
       "  'start': 1737.679,\n",
       "  'duration': 4.48},\n",
       " {'text': 'answer. Now I can be dumb and just show',\n",
       "  'start': 1740.32,\n",
       "  'duration': 4.56},\n",
       " {'text': 'this to the user or I can use this to I',\n",
       "  'start': 1742.159,\n",
       "  'duration': 4.721},\n",
       " {'text': 'make a decision, call another A2A agent',\n",
       "  'start': 1744.88,\n",
       "  'duration': 4.32},\n",
       " {'text': 'or whatever. You see how explicit A2A',\n",
       "  'start': 1746.88,\n",
       "  'duration': 5.12},\n",
       " {'text': 'is. The agent has first class capability',\n",
       "  'start': 1749.2,\n",
       "  'duration': 4.959},\n",
       " {'text': 'to request for additional input. And we',\n",
       "  'start': 1752.0,\n",
       "  'duration': 4.08},\n",
       " {'text': 'saw that in our example as well when we',\n",
       "  'start': 1754.159,\n",
       "  'duration': 3.52},\n",
       " {'text': 'were trying to schedule some tasks with',\n",
       "  'start': 1756.08,\n",
       "  'duration': 3.52},\n",
       " {'text': 'the calendar. Essentially what was',\n",
       "  'start': 1757.679,\n",
       "  'duration': 3.441},\n",
       " {'text': 'happening is the calendar did not have a',\n",
       "  'start': 1759.6,\n",
       "  'duration': 5.36},\n",
       " {'text': '4hour block free. So it asks a question.',\n",
       "  'start': 1761.12,\n",
       "  'duration': 6.24},\n",
       " {'text': 'It sends back a message saying hey I',\n",
       "  'start': 1764.96,\n",
       "  'duration': 4.4},\n",
       " {'text': 'need some more questions. Again the user',\n",
       "  'start': 1767.36,\n",
       "  'duration': 3.6},\n",
       " {'text': 'could just you know take that and handle',\n",
       "  'start': 1769.36,\n",
       "  'duration': 3.52},\n",
       " {'text': 'it directly. But in our case our',\n",
       "  'start': 1770.96,\n",
       "  'duration': 3.599},\n",
       " {'text': 'assistance was intelligent. Again',\n",
       "  'start': 1772.88,\n",
       "  'duration': 2.96},\n",
       " {'text': \"there's nothing wrong with intelligence\",\n",
       "  'start': 1774.559,\n",
       "  'duration': 3.12},\n",
       " {'text': 'right? the the assistance was', 'start': 1775.84, 'duration': 3.28},\n",
       " {'text': 'intelligent and said that hey if we',\n",
       "  'start': 1777.679,\n",
       "  'duration': 3.921},\n",
       " {'text': \"don't have 4hour task free you know we\",\n",
       "  'start': 1779.12,\n",
       "  'duration': 4.32},\n",
       " {'text': 'can just ask notion to break that task',\n",
       "  'start': 1781.6,\n",
       "  'duration': 4.079},\n",
       " {'text': 'down how does it know that notion can',\n",
       "  'start': 1783.44,\n",
       "  'duration': 4.479},\n",
       " {'text': 'break that task down because in notion',\n",
       "  'start': 1785.679,\n",
       "  'duration': 4.161},\n",
       " {'text': 'skill there would be one example which',\n",
       "  'start': 1787.919,\n",
       "  'duration': 3.041},\n",
       " {'text': 'states that you know what you can give',\n",
       "  'start': 1789.84,\n",
       "  'duration': 2.559},\n",
       " {'text': 'me a task and I can break it down for',\n",
       "  'start': 1790.96,\n",
       "  'duration': 4.16},\n",
       " {'text': \"you so the assistant doesn't need to be\",\n",
       "  'start': 1792.399,\n",
       "  'duration': 5.28},\n",
       " {'text': \"like an extremely good planner notion's\",\n",
       "  'start': 1795.12,\n",
       "  'duration': 5.36},\n",
       " {'text': \"skills or the notion's agent card will\",\n",
       "  'start': 1797.679,\n",
       "  'duration': 4.24},\n",
       " {'text': 'be explicit saying that hey you know',\n",
       "  'start': 1800.48,\n",
       "  'duration': 3.12},\n",
       " {'text': 'what you can break things down so yes',\n",
       "  'start': 1801.919,\n",
       "  'duration': 3.601},\n",
       " {'text': 'you have intelligence in this case but',\n",
       "  'start': 1803.6,\n",
       "  'duration': 3.799},\n",
       " {'text': 'you know exact ly when to use that',\n",
       "  'start': 1805.52,\n",
       "  'duration': 4.159},\n",
       " {'text': \"intelligence. It's very explicit that\",\n",
       "  'start': 1807.399,\n",
       "  'duration': 4.121},\n",
       " {'text': 'input is required, use some intelligence',\n",
       "  'start': 1809.679,\n",
       "  'duration': 3.681},\n",
       " {'text': 'to solve it. If you cannot figure out a',\n",
       "  'start': 1811.52,\n",
       "  'duration': 4.56},\n",
       " {'text': 'way out, then ask the user whatever. And',\n",
       "  'start': 1813.36,\n",
       "  'duration': 4.72},\n",
       " {'text': 'once the task is actually done, you get',\n",
       "  'start': 1816.08,\n",
       "  'duration': 3.52},\n",
       " {'text': 'an artifact back. So in this case, you',\n",
       "  'start': 1818.08,\n",
       "  'duration': 2.88},\n",
       " {'text': 'would get an artifact back and now you',\n",
       "  'start': 1819.6,\n",
       "  'duration': 3.199},\n",
       " {'text': \"know it's done. Again, A2A allows you to\",\n",
       "  'start': 1820.96,\n",
       "  'duration': 3.36},\n",
       " {'text': \"follow up on artifacts as well. That's\",\n",
       "  'start': 1822.799,\n",
       "  'duration': 2.801},\n",
       " {'text': 'another conversation. Check out the',\n",
       "  'start': 1824.32,\n",
       "  'duration': 3.359},\n",
       " {'text': \"previous video. Cool. So the point I'm\",\n",
       "  'start': 1825.6,\n",
       "  'duration': 3.36},\n",
       " {'text': 'trying to make, support for', 'start': 1827.679, 'duration': 3.6},\n",
       " {'text': 'collaboration. What does A2A say? Hell',\n",
       "  'start': 1828.96,\n",
       "  'duration': 4.04},\n",
       " {'text': 'yeah, that is', 'start': 1831.279, 'duration': 4.161},\n",
       " {'text': \"better. I'm just enjoying myself today.\",\n",
       "  'start': 1833.0,\n",
       "  'duration': 4.6},\n",
       " {'text': \"I'm having fun on this stream. What does\",\n",
       "  'start': 1835.44,\n",
       "  'duration': 6.64},\n",
       " {'text': 'MCP say? Yeah, but you got to do it',\n",
       "  'start': 1837.6,\n",
       "  'duration': 6.24},\n",
       " {'text': \"yourself. Like you'll have to architect\",\n",
       "  'start': 1842.08,\n",
       "  'duration': 3.599},\n",
       " {'text': \"the tools and stuff yourself. So it's\",\n",
       "  'start': 1843.84,\n",
       "  'duration': 3.92},\n",
       " {'text': 'yeah with an asterk you can do it. It',\n",
       "  'start': 1845.679,\n",
       "  'duration': 6.401},\n",
       " {'text': \"just it's not as seamless as A2A. Okay.\",\n",
       "  'start': 1847.76,\n",
       "  'duration': 6.0},\n",
       " {'text': 'And this collaboration is a big one. Not',\n",
       "  'start': 1852.08,\n",
       "  'duration': 4.4},\n",
       " {'text': 'just that this is something for me. So',\n",
       "  'start': 1853.76,\n",
       "  'duration': 3.919},\n",
       " {'text': 'the point is whenever you have',\n",
       "  'start': 1856.48,\n",
       "  'duration': 2.88},\n",
       " {'text': \"intelligent agent that means there's an\",\n",
       "  'start': 1857.679,\n",
       "  'duration': 3.201},\n",
       " {'text': 'LLM somewhere in the middle, right? And',\n",
       "  'start': 1859.36,\n",
       "  'duration': 3.199},\n",
       " {'text': 'again for a good experience you know if',\n",
       "  'start': 1860.88,\n",
       "  'duration': 3.76},\n",
       " {'text': \"you're returning the LLM's output to the\",\n",
       "  'start': 1862.559,\n",
       "  'duration': 4.24},\n",
       " {'text': 'user you want to stream the output right',\n",
       "  'start': 1864.64,\n",
       "  'duration': 5.039},\n",
       " {'text': 'so a good user experience requires',\n",
       "  'start': 1866.799,\n",
       "  'duration': 5.12},\n",
       " {'text': 'streaming like as the response is being',\n",
       "  'start': 1869.679,\n",
       "  'duration': 3.841},\n",
       " {'text': 'generated by the server you want to see',\n",
       "  'start': 1871.919,\n",
       "  'duration': 3.601},\n",
       " {'text': 'it be typed like how we see it with chat',\n",
       "  'start': 1873.52,\n",
       "  'duration': 5.279},\n",
       " {'text': 'GPT for that matter right so in that',\n",
       "  'start': 1875.52,\n",
       "  'duration': 5.279},\n",
       " {'text': 'case what can happen is again MCP',\n",
       "  'start': 1878.799,\n",
       "  'duration': 3.76},\n",
       " {'text': 'supports streaming just how you would',\n",
       "  'start': 1880.799,\n",
       "  'duration': 4.24},\n",
       " {'text': 'stream with uh Win AI spec you know just',\n",
       "  'start': 1882.559,\n",
       "  'duration': 4.72},\n",
       " {'text': 'say stream true and the best part is',\n",
       "  'start': 1885.039,\n",
       "  'duration': 4.801},\n",
       " {'text': \"again it's so freaking explicit because\",\n",
       "  'start': 1887.279,\n",
       "  'duration': 4.961},\n",
       " {'text': 'in your agent to agent card. The agent',\n",
       "  'start': 1889.84,\n",
       "  'duration': 4.4},\n",
       " {'text': 'explicitly says I support streaming,',\n",
       "  'start': 1892.24,\n",
       "  'duration': 4.0},\n",
       " {'text': 'right? Which is great. So instead of',\n",
       "  'start': 1894.24,\n",
       "  'duration': 3.84},\n",
       " {'text': 'submitting a task now you say I want to',\n",
       "  'start': 1896.24,\n",
       "  'duration': 3.439},\n",
       " {'text': 'submit subscribe and then you could just',\n",
       "  'start': 1898.08,\n",
       "  'duration': 3.28},\n",
       " {'text': 'say hey this is something some response',\n",
       "  'start': 1899.679,\n",
       "  'duration': 3.281},\n",
       " {'text': 'I want from you and now instead of',\n",
       "  'start': 1901.36,\n",
       "  'duration': 3.439},\n",
       " {'text': 'saying completed the server sends back a',\n",
       "  'start': 1902.96,\n",
       "  'duration': 3.439},\n",
       " {'text': 'status of working because again it was',\n",
       "  'start': 1904.799,\n",
       "  'duration': 3.36},\n",
       " {'text': 'send subscribe instead of just send task',\n",
       "  'start': 1906.399,\n",
       "  'duration': 3.041},\n",
       " {'text': \"and now you're just getting your stream\",\n",
       "  'start': 1908.159,\n",
       "  'duration': 3.201},\n",
       " {'text': \"back and it's so cool. Just look at it.\",\n",
       "  'start': 1909.44,\n",
       "  'duration': 3.76},\n",
       " {'text': \"It's saying that okay the answer I'm\",\n",
       "  'start': 1911.36,\n",
       "  'duration': 2.88},\n",
       " {'text': 'going to give back is finally an',\n",
       "  'start': 1913.2,\n",
       "  'duration': 2.16},\n",
       " {'text': \"artifact. So it's not a follow-up\",\n",
       "  'start': 1914.24,\n",
       "  'duration': 2.4},\n",
       " {'text': \"question. It's directly an artifact.\",\n",
       "  'start': 1915.36,\n",
       "  'duration': 3.679},\n",
       " {'text': \"This is my section. Don't append this\",\n",
       "  'start': 1916.64,\n",
       "  'duration': 4.32},\n",
       " {'text': 'cuz remember when you use OpenAI',\n",
       "  'start': 1919.039,\n",
       "  'duration': 4.161},\n",
       " {'text': 'streaming you get delta chunks back. So',\n",
       "  'start': 1920.96,\n",
       "  'duration': 3.92},\n",
       " {'text': 'you need to like append it on your own',\n",
       "  'start': 1923.2,\n",
       "  'duration': 3.28},\n",
       " {'text': 'end and you know things like that. But',\n",
       "  'start': 1924.88,\n",
       "  'duration': 3.2},\n",
       " {'text': 'some servers might give you the entire',\n",
       "  'start': 1926.48,\n",
       "  'duration': 2.96},\n",
       " {'text': \"response back. You don't need to append\",\n",
       "  'start': 1928.08,\n",
       "  'duration': 3.76},\n",
       " {'text': 'it. So here the response is explicit.',\n",
       "  'start': 1929.44,\n",
       "  'duration': 4.4},\n",
       " {'text': \"It's saying that okay this chunk don't\",\n",
       "  'start': 1931.84,\n",
       "  'duration': 3.76},\n",
       " {'text': 'append it. Meaning this chunk gets',\n",
       "  'start': 1933.84,\n",
       "  'duration': 3.439},\n",
       " {'text': 'replaced. So whatever you had in your',\n",
       "  'start': 1935.6,\n",
       "  'duration': 4.079},\n",
       " {'text': 'buffer replace it with this. And then',\n",
       "  'start': 1937.279,\n",
       "  'duration': 4.0},\n",
       " {'text': 'when you get the next chunk you see here',\n",
       "  'start': 1939.679,\n",
       "  'duration': 3.681},\n",
       " {'text': \"it's append. So now what it's saying is\",\n",
       "  'start': 1941.279,\n",
       "  'duration': 3.76},\n",
       " {'text': 'that this next section needs to be',\n",
       "  'start': 1943.36,\n",
       "  'duration': 3.439},\n",
       " {'text': 'appended to your previous array. So',\n",
       "  'start': 1945.039,\n",
       "  'duration': 3.681},\n",
       " {'text': \"you're adding it again if the append was\",\n",
       "  'start': 1946.799,\n",
       "  'duration': 3.681},\n",
       " {'text': 'false in that case you would replace it',\n",
       "  'start': 1948.72,\n",
       "  'duration': 3.28},\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598f289",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6503018a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e816b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \" \".join(chunk['text'] for chunk in raw_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ceb9fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, Google just released its agent to aagent protocol, but with its launch, there are some big questions which arise like what about MCP? Like does A2A replace MCP? Is there an overlap? And if not, when do you use what? Well, in this stream, we\\'re going to pit A2A against MCP and see what their differences are. Take a good example to highlight some use cases and finally arrive at a framework which helps us decide when to use what. Let\\'s get started. I think the best way to compare any frameworks is by looking at an example, right? And we already saw this, so let\\'s just go through it again. It can be a quick recap. Okay, let\\'s say that I\\'m a human. Yeah, a cute little guy. That\\'s me. Yep. Right here. And let\\'s say I\\'m supposed to finish a project by the end of the week, right? And I keep all my project stuff in notion and I keep all my calendar in Google calendar, right? So my projects and my tasks are in notion and everything else is in Google calendar. And I just want to make sure that all my tasks for a particular project gets done by the end of the week. Why? Cuz my manager is annoying. So yeah, need to finish it by the end of the week. And we start with a request. We tell our personal assistant that hey you know what schedule all the task for project A by the end of this week. Now the assistant will go ahead to notion and will be like hey give me all the task for this project with estimates. Notion will give me back a response. It will say that hey you have two task task A is 1 hour task B is 4 hours. Then the assistant goes ahead to Google calendar and goes like hey you know what can you now schedule these tasks in this week itself. I have two task just be kind to me. So obviously Google calendar tries its best but unfortunately you know we don\\'t have enough space in our calendar. We just don\\'t have a 4-hour block free. I\\'m a busy man guys. So yeah we get a response back. Now what do we do? So let\\'s say the assistant takes a call that hey can we break task B down? So instead of scheduling it in one shot can we break it down into subtasks? So we go to notion and we say hey can we break this task further down? and notion goes like, \"Oh, you know what? We actually can.\" And then it gives a list of subtask or something as well. And then we go ahead to the calendar and be like, \"Hey, can you schedule this instead?\" And it says, \"Hey, I scheduled it.\" And while all of this was happening, because our agent looks like this, our agent is impatient. And you know, in my previous video, users are monkeys. They\\'re impatient beings. So, you need to make sure, you need to comfort them that, hey, something is happening. So, they stick along. You know, they just stay with you. They don\\'t leave you. So what you do is as things are happening in this agent, you need to start giving it updates, right? You need to give it updates to build a good user experience that hey, you know what? I got two tasks back. Oh, wait a minute. Or maybe you could have an update saying now I\\'m trying to schedule it. Maybe there could be another update to schedule it. Maybe we should add that. Trying to schedule now. Uh this should come before scheduling. Don\\'t worry about the timing in the sequence diagram. It\\'s all over the place. Okay, but you get the gist, right? You\\'re giving your user updates on what is happening. So the user is in loop. This will help the user know that okay my agent is actually doing the right thing which is kind of important and not just that I mean you could even have some human and loop scenarios right where you could be like okay we want to break a task down and notion was like yeah you can break this task down but what if notion doesn\\'t know how to break the task down what if it comes up with incorrect subtasks maybe we want to show the user an input hey we want to break task B down can we do that and the user goes like yeah cool you can do this and then you schedule it so You have like a a little human in loop scenario going on in here. That\\'s how the real world operates, right? Your agents might screw things up or some things might not be possible. So, you need like follow-up questions. And then you also want to give feedback to the user. You also want like human and loop scenarios and all that funny stuff. That\\'s how real agent interaction works, at least in the real world, right? This is the example we\\'re going to use to start comparing things. Sounds good. You guys are with me? Okay. One important point. I\\'m going to skip some lowhanging fruits like I\\'m going to skip o authentication. I\\'m going to skip service discovery. I know for some people this was like a big deal that oh my god MCP doesn\\'t support O or MCP doesn\\'t have a service discovery API or whatever. Those are not that big problems because I mean you could just shove in tokens in the headers and things like that like you do with REST APIs anyways. Those were not that big problems at least according to me. So we\\'re going to skip it just you know for clarity sake. A2A does support authentication and authorization and in fact it just uses the open API authentication. again open API not open AI it\\'s open API so it uses the open API\\'s authentication specification which essentially says the agent will explicitly mention that hey I support basic authentication which is username password I support better token authentication I support O2 blah blah blah so the agent will be explicit about what authentication it supports but you still like have to give it up front I know this is on uh MCP\\'s road map as well not just that A2A also has a discovery document so it actually says says that oh you know what if you want to discover agents you can do it using well-known format which we usually use like sitemaps and things like that you could also have like an API they clearly mentioned that hey we don\\'t have a protocol for the registry yet but if you need it then let us know because I agree this is not that important you can make it yourself why do you need a protocol for these things but okay that\\'s the point these are lowhanging fruits these are things which do not really impact functionality that much I mean they\\'re important you have to implement them but I\\'m going to skim those things so Let\\'s talk about the comparison. And how are we going to do that? We\\'re going to make a table. Let\\'s try to do this. I think this is where we should start. Okay. So, the way we\\'re going to do this is we\\'re going to write A2A here. We\\'re going to write MCP here. And then each feature we\\'ll write here. So, against every feature, we\\'ll see what A2A does for it, what MCP does for it, right? Cuz on the surface, it seems like there\\'s a lot of overlap, but it\\'s actually not. Spoiler alert, they\\'re very different protocols for very different purposes. Yeah. Cool. So the first thing I want to start with is capability discovery. So how do you discover what is the possible options or what are the different functionalities which an A2A server or an MCP server give you right like how did you know in this example that hey you can get these tasks like you can even ask it for task while asking for task you can also ask for estimates how do you know those things right so there has to be some way to discover what capabilities or what functionality does this guy give us that\\'s that\\'s kind of important yeah so for a toa we have something called skills the way we discover cap capabilities in A2A is through skills. Way we do it in MCP is through tools. Now, technically MCP does have resources and prompts as well. I\\'m just going to skip those guys. I don\\'t know who\\'s using it, but uh yeah, we\\'re just going to limit our thing to tools and skills because I think that\\'s where a lot of the mixup would happen. Prompts and resources is like a completely different thing. So, I\\'m just going to skip those things. So, in A2A, you have the concept of a skill. In MCP, you have a concept of a tool. So we already saw an example for A2A but let\\'s go through that again. Let\\'s just look at discovery first and then we can say who can use it right. So the first how it works for A2A. You have something called the agent card and this agent card is on steroids. This is good stuff. This is what you can discover. So every agent publishes this card. It\\'s just a JSON object and it declares that hey this is my capability right. So you can clearly see that okay as input it supports plain text and as output it can do plain text as well and it can even create HTML code as output. In capabilities it says it can support streaming. So it supports streaming responses. It supports something called push notifications. We won\\'t talk about push notifications. See the previous video for that one. Now this is the most important thing. Oh there\\'s also one more thing. There\\'s something called task state updates or something like that. That\\'s also a very cool capability. So there are three capabilities right now in the spec. Again check out the previous video for all this. Now this is the cool stuff. Unlike MCP, A2A has skills, right? This is a Google maps agent, right? So the skill is route planning. It can do route planning. It helps planning routes between two locations. Now this is a very generic description. What does it actually mean? It\\'s got examples right here which says plan my route from Sunnyale to Mountain View. Oh, so route planning means actually planning my route from Sunnyale to Mountain View. Okay, interesting. What does route planning mean? What\\'s the commute time from Sunnyale to San Francisco at 9:00 a.m.? Oh, it\\'s being so specific here. So, not only can I use it to plan my routes, I can also use it to get the time for the routes. Like, how much time does it take? Maybe I know the route. I just want to know how much time would it take for a particular hour, something like that. You can do that, too. All right, cool. I\\'m learning new things. And then you can get turnbyturn directions. So, one route could be just, hey, here to here. And maybe another could be a turnbyturn route where you get like a very comprehensive thing like, okay, go left from here, go right from here. The point is it\\'s actually telling you very high level broad capabilities or queries you can ask this agent right and it\\'s very natural language focused I think that\\'s a really important point again I don\\'t know how to best describe this I\\'m so sorry about that but in A2A skills are really broad capabilities with some examples on how you can mess around with it there is intelligence on the back end which is trying its best to take your query and give you an answer on it so under the hood technically speaking this one thing could require 10 tool calls maybe. I don\\'t know how many tool calls would Google have to do to answer this. I don\\'t know how many tool calls would Google have to do to answer that. The point is I just give it questions in natural language and I don\\'t have to worry about the breaking down of questions and things like that. I can just give it questions and give me some example questions as well. Thank you. And obviously you can have as many skills as you want. So that\\'s how you define skills in A2A at least. Okay. In MCP you do tools. So how would it look like in MCP? Okay, so this is how you do it in MCP. Oh boy. So you define tools. There\\'s a very fixed scope of what a tool is. So in this example, the tool is calculate sum. There\\'s a description. There\\'s a very strict input that hey for the input you have to give me a number. You have to give me another number and you know both are required. I mean it doesn\\'t describe the output but it describes the input. It\\'s very very fixed right. And all you can do is you get an API saying give me all the tools and that will essentially give you an array of these tools. So you could see there\\'s a tool for calculate sum there could be a tool for add two numbers it could be for everything. So again going back to this example which we had here if this was an MCP server instead of notion agent. So instead of getting skills back with example queries you can ask you would get a list of tools back and one tool would be get tasks with estimates. One tool could be just get tasks right one tool could be get estimate for task right. So those would be tools. You would have five six different tools and somebody would have to decide what tool to call. A2A is not explicit at all. A2A just gives you a list of skills and you just give it a query. You don\\'t even have to say whether I want to use skill one or skill 2. You just give it a query and the agent will figure everything out by itself. That\\'s wild, right? In MCP very well defined in A28 is a little bit more broad. Right? Some clarifications. I don\\'t want to make you think that wait that A2A is more broad and MCP cannot be broad. MCP can be broad as well. Let\\'s just uh drill down on this a little bit more. Okay. So basically what I\\'m suggesting is in MCP you have like get tasks right and then for get tax you could take a project as an input and you give some output right that\\'s an example of how the tool would look like and technically what I\\'m saying here that this will only have one function let\\'s call it uh send task and this could be anything this is just a message which could be any and by any I mean obviously there\\'s some protocols around it it could be a string it could be a image it could be a lot of different things but that\\'s what I\\'m saying right but technically in MCP as well you could have the same kind of a function, right? You could technically just have one tool in MCP which says send task and the message could be any. It could be string, it could be binary, it could be anything. And now you have the same kind of API for both of them, right? So A2A is kind of like MCP with a single tool. That\\'s what I\\'m trying to highlight. A2A is like MCP server with a single tool where you just take a message as the input and the message could be anything. It could be natural language string. It could be structured. It could be a JSON object. It could literally be anything. However, most people will use MCP more like this like get tasks uh create task and stuff like that, right? Create task and maybe you have a task as well, whatever. You get the point, guys. Again, A2A is like an MCP server with just a single tool. All right? However, in MCP, the spec currently at least currently only has a description. So, the only way you can describe what this tool does is through a description. So you would have to write like a really huge description with a lot of examples. So it would become a very difficult process to do it. I\\'m saying you can do it but it\\'s really complex. So having a single tool which can do everything in MCP is going to be a little bit hard because your description is going to be bonkers. It\\'s going to be really hard. Instead in A2A like we just spoke you have skills and skills has a first class concept of example. So this is much tidier. This is much neater. In fact you don\\'t even talk about functions. You\\'re just like hey I just have one API send task. You give me whatever you want and these are some examples on what you can do with me. What you can do with me? That doesn\\'t sound right. Cool. Sweet. That\\'s the capability discovery. I hope this makes sense. There\\'s one more thing I want to highlight. Let\\'s say in an MCP server, we have something called create task and it takes a project and a task. Okay, let\\'s say I have an MCP server which takes two arguments. Now, what if the user\\'s original question was like create this task and they do not mention a project. What would happen? Think about that. What would happen if the user shows intent to call a particular tool but doesn\\'t give you all the inputs? In that case, your client, this guy would have to be smart enough to say that okay, to call this MCP tool, I need some more input. So, I need to ask for more input from the user. Right? The client would have to be smart saying that okay, you know what to call this MCB tool, I need two parameters. The user\\'s question just has one parameter. So, I have to go back to the user and say that hey, you know, user uh you need to give me some more information. So there\\'s a lot of intelligence which has to sit on the MCP client and you know we\\'ll talk about intelligence soon as intelligence is going to be a very big one. I hope you get it. The point I\\'m trying to convey is in case of MCP it\\'s the client\\'s responsibility to populate all the parameters to be called. In case of A2A you can literally pass it everything. You can even give it an incomplete input. You can technically still give it an incomplete input and it would still work cuz A2A has built-in mechanism where the agent will ask you for further clarification. Right? So your client doesn\\'t have to worry about it so much. Okay. Important point. And the biggest point I want to highlight here is in case of A2A intelligence moves from the server to the client. So intelligence mostly on the server mostly. I\\'m not saying your client cannot be intelligent. I\\'m saying in most use cases like 80% of the use cases your client are not going to be as intelligent. Your servers are going to be more intelligent. So your client will just be like hey the user asked me this question. Can you give me an answer to this question? So the client is just like passing things around. The server will be like wait I need more clarification. The client will be like oh you need more clarification I\\'ll just ask the user for more clarification. So the client is just like the middleman you know it doesn\\'t have a lot of intelligence. I\\'m not saying it cannot it can. There could be use cases which demand it but mostly the intelligence will be on the server. For MCP mostly the intelligence will be on the client. Your client will have to figure out whether there are some parameters missing or not. The client will have to figure out you know what we need some more input. Your client will have to even plan what tools to call in what format. So the client is doing a lot more work than a A2A client, right? So in MCP you need a smarter client. So intelligent is mostly on the client. The servers are dumb. The servers are just like, \"Yeah, I\\'ll do a database call. Yeah, I\\'ll do a create task.\" But for A2A, it\\'s the other way around. However, again, highlight this. I know I\\'ve said this like 10 times already. But for MCP, you could have A2A like semantics and MCP as well. Like I said already, you could have just one tool which says send task. You could have a tool like this over here as well, you know, which is smart. But again, then you fall short. Like what if the input wasn\\'t clear? You\\'ll have a function which gives you back a response saying that actually I need more input. Right? So the function response for this in that case would be wait a minute you did not give me the project ID. Give me a project ID as well or give me the project name as well. But now the client would have to be smart enough to say wait the answer I got back is not the final answer. It\\'s actually a prompt for asking something more. So again even if you try to move intelligence to the MCP server your client will still have to be smart enough to understand that hey you know what the response I\\'m getting back is actually not the answer. It\\'s a follow-up question. So, you can make A2A using MCP. I\\'m just saying it\\'s really convoluted to do that. Does that make sense? Explorer brings a point that what if I have an MCP server to do research. So, it essentially takes a research. It takes a user prompt. It does research and gives back an answer. So, Explorer is arguing that the server needs to be intelligent in that case. However, I would argue let\\'s take a real world. Let\\'s try to extrapolate it at you know add some more features to it. Let\\'s say your researcher agent first plans, confirms the plan from the end user and then does the research, right? I think that\\'s how Google Gemini does its research and I really love it cuz it shows me the plan and I can iterate on the plan and then actually do the research. It makes I like it. Let\\'s say you want to add that feature, right? In that case, you would have to create two tools. You would have to say, okay, first I want a research plan, then you would have to say research. But your research plan will also have to be smart because what happens if you give back a plan, but the user is like I want to iterate on the plan. So your plan will be something like this I guess right that okay what was the past messages like a message array or something like that that okay this was the initial plan then the user\\'s asking a follow-up question so I mean you can do it you can model all of that conversation it\\'s just so convoluted and then obviously once your plan is done then you call the research right so I\\'m saying you can do it with MCP I\\'m just saying that there\\'s a lot of back and forth happening and there\\'s a lot of modeling which will have to happen and you\\'ll have to create these layers right which is annoying I am arguing something I\\'m arguing arguing don\\'t do that okay just have a get tasks in your example I would do this I would have MCP server and what would this MCP server do this MCP server would have some tools Google search knowledgebased search that\\'s it that\\'s what my MCP server would do I would have an A2A server saying I have a skill cuz remember we have skills here so I\\'m just writing it out so we have a skill uh MCP server tools and A2A server skills I would say I would have an A2A server with skills and I would say my skill is what I do research and I\\'ve got some examples find me research papers on something from Google right you could have another example saying again remember examples is a part of the official spec okay I\\'m not making things up this is part of the spec I could have another example answer this question from my company knowledge base you see the difference here so over here I\\'m saying hey this is a skill I do research I have these sample things I can search from Google I can search from knowledgebased and this server internally would be using MCP tools to do the Google search and knowledgebased search. Again, I\\'m saying this server could be an MCP server, but A2A just seems like a better way to do it cuz A2A has, you know what, we\\'re getting into the comparison ahead. My job is to prove why A2A is a better way to do this. So, instead of having this research as a tool and an MCP server, I\\'m going to argue why the A2A server needs to be a skill. So, the research thing should be an A2A server and why these things should be MCP tools, right? That\\'s the argument I\\'m going to make. But still the point is that an MCP server can be smart too. Client mostly discover and then you have a router on the client. Yeah, that\\'s right. You could have smart MCP servers too. That\\'s why I use the words mostly. So intelligence is mostly on the client. That\\'s what I recommend. Uh but yeah, you could have some edge cases where your MCP server really needs to be smart as well. Why not? I mean you want to do it, you can do it, right? If you don\\'t make MCP servers which are smart, A2A will eat MCP servers for lunch with the A2A protocol. That\\'s precisely what I\\'m going to argue. I\\'m going to argue that any MCP server which had intelligence which was using an LLM should and will be replaced by an A2A server. That\\'s my hot take and that\\'s what I\\'m going to prove in this comparison. It\\'s opinionated but hey that\\'s my thing. Okay. What\\'s the point of MCP server if you could make A2A server with similar task because A2A doesn\\'t have the schema approach. There is no way in A2A to say that okay you know what I need an input with these precise things. So if you want to do a Google search or something like that, you know these parameters, A2A has no way to describe the schema for those parameters. A2A could add that in their specification. Technically speaking, they could add capability called tools in their specs, but they haven\\'t. Maybe it\\'s to cozy up with MCP or not. I don\\'t know. Currently A2A has no way to give structured to define schemas for inputs. Which means if you want to call a database, if you want to like you know do any kind of structured tool call, A2A literally has no way to do it. There is no way to enforce a schema in the A2A protocol. So for tools, unfortunately, you just have to use MCP because A2A chose not to implement it. It could, it chose not to do it, right? So you still need MCP for that reason. But okay, let\\'s keep continuing. Let\\'s keep continuing. We\\'ve already given the end answer at this point. So now the stream has become like instead of arriving to this conclusion, we\\'re going to prove why is this conclusion a better conclusion? Oh my god, we just flipped the stream over. You guys always do that to me. Like my flow just goes for a toss. I mean, okay, cool. Yeah. So the next point I want to say is the task scope. So again, just covering what we already discussed. I\\'m going to actually put it up here. We already discussed this because the skills are so broad. The scope of A2A is not well defined. Right? You\\'re just giving examples. So you\\'re not really restricting the scope. You\\'re just like, \"Hey, a few examples. Stick with it.\" But there\\'s nothing forcing your client to stick with it, right? In this notion server, the client could technically say, \"Give me the scores for tonight\\'s baseball game.\" I don\\'t know. It could do that. There\\'s nothing stopping it. In MCP, there is no tools to get results of a baseball game. I don\\'t know even why I\\'m saying baseball. I don\\'t even watch baseball. But okay, in MCP, the scope is restricted to get task and creating task, right? In A2A there is no such way to do that which means your server is going to be complex. You need guardrails in A2A clearly it\\'s very well defined. I think one more thing goes in that in MCP input strictness and I think you\\'ll also get my thought process on like how am I comparing these things right? So input strictness not strict at all. It\\'s multimodal. I mean it\\'s free flow. Do whatever you want. Mostly strict. I also add mostly over here because I know we can have smart MCP servers man. I mean, you could technically have smart MCP servers. Why not? I mean, you could do it, but mostly it\\'s going to be strict, right? Because you\\'re defining the freaking type of every parameter you want, right? So, it\\'s as strict as it gets. This is a function call. MCP is just RPC. It\\'s remote procedural calls. Okay? Now, we\\'re going to get into the big stuff. So, right now, I\\'ve just been stating some lowhanging fruits. I\\'ll get into justifying this now. So, now comes a little a heavier portion. So, I want to take a pause here. Uh now I want to get into some big differences on why A2A servers should be intelligent. So I want to focus on this why intelligent servers should be A2A and why dumb servers should be MCP. Okay, that\\'s what we need to get at. Let\\'s start with the obvious point which we\\'ve already covered. I\\'ll just reiterate it. If you have an MCP for notion, right, which does create get task and create task or whatever, somebody has to make a plan. Let\\'s take a simpler example which I think a lot of people have seen out in the world. Let\\'s say you have an MCP server for a database cuz who isn\\'t doing this, right? Nobody\\'s doing this. Nobody\\'s building MCP servers for databases, right? I mean, that\\'s absurd. Who would do that? I\\'m kidding. So, yeah. So, let\\'s say we have a database. Okay. And your database has a couple of tools. So, it has a tool called get schema and it has a tool called run query, right? So, get schema gets you the schema of the database. Run query takes a SQL query and runs it. the MCP client, the client which is sitting here would have to be smart enough to first decide that you know what first I need to get the schema based on the schema I need to generate the query and then run the query something like that right no matter what your MCP server is you\\'ll always need some kind of intelligence now like also we discussed you could have something like this you could have an MCP tool which is essentially run natural language query in this it seems like the client is becoming dumb again right like it can just do this correct uh it can just take the natural language and pass this and this works Don\\'t get me wrong, this works. You could have an MCP server which, you know, gets the schema, does all that fun stuff and, you know, then calls the database. So, yeah, you could technically have an MCP server which does this, but once you start getting into the real world examples, right, and you figure out something like the user\\'s asking a question about tasks, but there is no table called tasks. Now, this tool will suddenly give you a response back saying, \"Wait a minute. Know what table you\\'re talking about? Did you mean this?\" Right? Something like that. Wait a minute. I don\\'t know what you\\'re talking about. Did you mean this? Or maybe the MCP server says, \"Okay, I want to run this query. Is that fine?\" Right? So, or it could be like because it\\'s a read query, here\\'s the results of the read query. You see what just happened here? We\\'re breaking it down. So, either we could have a situation wherein if it was a read query, we directly get the result of the query. If it was a write query, then we say, \"Hey, we want to run this query. Is that fine?\" And if you give me some very stupid query, then I\\'ll be like, \"Hey, I don\\'t even know what you\\'re talking about. Are you talking about something like this instead?\" How do you model this? So the moment you get a response back, now your client will need intelligence cuz your client will have to say, \"Wait, the response I got back, is that response a final response? Is that response something the human has to confirm? Is that response something I need to iterate on myself?\" How do you know? You need intelligence just to understand whether the task was completed or not. And now again you\\'ll be like but wait you know I can have a separate tool for write queries a separate tool for read queries again you\\'ll have to decide which tool to call and your client becomes intelligent. The point is because MCP is just a request response format the client will either have to pay attention to what tool to call. You\\'ll need intelligence in figuring out what tool to call or you will need intelligence on how to parse the response. So you need intelligence for one of these two things just because of the nature of MCP. Again, I\\'m not saying having intelligence on the client is a bad thing. I\\'m not saying that at all. I\\'m just saying that this is an additional step you\\'ll have to worry about, right? You\\'ll have to worry about figuring out which tool to call or you\\'ll have to worry about interpreting the response. It\\'s not clear. Let\\'s see how A2A does this. And A2 has a really good example in the specification. So I\\'m just going to write that and then if you want we can extrapolate it further together. Let\\'s say we have a question saying that okay request a new phone or run this query whatever right multiple things can happen. So okay wait wait wait wait wait wait wait sorry I need to backtrack because I know this is a new video so we did not speak about it so let\\'s speak about this first in MCP you have something called tasks right so you submit tasks to users again if you want a detailed deep dive on the A2A spec check out my previous video but essentially you submit task and task could be like anything it\\'s natural language it could be an image it could be anything so you essentially submit a task right now the server might be able to finish that task for you straight up right if it\\'s a read query it could finish it straight up or it could be like wait I have some more input to ask you correct so let\\'s say in this case there\\'s more information to ask so now what happens is the server tells you back that hey wait a minute input required there\\'s an explicit message called input required or there\\'s an explicit state called input required with a very precise question so A2A is explicit it\\'s saying that I did get something back from my server but it\\'s not a response you don\\'t have to figure out whether I got the response or not it explicitly saying, \"Hey, you know what? I need some more input and this is where now I could have a dumb client which could just take this and give it to the user directly on the UI or it could be a smarter client which uses intelligence to do something fun.\" Again, there\\'s nothing wrong with intelligence on the client. I\\'m just saying you don\\'t have to figure out whether the task was complete or not. So, obviously, let\\'s say, you know, our client is dumb in this case, so it just spits it on the user and the user just types in android on the UI, right? So, again, so you send this question to uh to the server. Again, you\\'re just submitting the task against the same task ID. So, you\\'re like you\\'re continuing a conversation with the server, right? So, you\\'re just saying, \"Hey, I want to continue the conversation with the same task which we just started with earlier.\" Uh, and I\\'m just saying Android. Again, I\\'m not giving anything else. Remember, it\\'s like a conversation. So, I don\\'t need to be explicit. I\\'m just answering this question. Now, the server says state is completed. Very explicit, extremely explicit that, hey, I\\'m done. Now, I have an answer. We\\'re done. Okay? And it\\'s not giving us a message back. Remember in the earlier thing it was it was giving us a message back. It was saying hey this is my response to the task. My status is input required and I have a message for you. I\\'m conversing with you. I am participating in a conversation or I\\'m collaborating with you to complete the task. Right? So this is a message is a collaboration. However once your task is done you get an artifact that hey order confirmation. So we have an artifact called order confirmation and you can have some details. If you were generating an image, the artifact would be the image. If you were creating a task, the task ID could be the artifact. The point is once you get the artifact, you know for sure that hey, things are done. I have an answer. Now I can be dumb and just show this to the user or I can use this to I make a decision, call another A2A agent or whatever. You see how explicit A2A is. The agent has first class capability to request for additional input. And we saw that in our example as well when we were trying to schedule some tasks with the calendar. Essentially what was happening is the calendar did not have a 4hour block free. So it asks a question. It sends back a message saying hey I need some more questions. Again the user could just you know take that and handle it directly. But in our case our assistance was intelligent. Again there\\'s nothing wrong with intelligence right? the the assistance was intelligent and said that hey if we don\\'t have 4hour task free you know we can just ask notion to break that task down how does it know that notion can break that task down because in notion skill there would be one example which states that you know what you can give me a task and I can break it down for you so the assistant doesn\\'t need to be like an extremely good planner notion\\'s skills or the notion\\'s agent card will be explicit saying that hey you know what you can break things down so yes you have intelligence in this case but you know exact ly when to use that intelligence. It\\'s very explicit that input is required, use some intelligence to solve it. If you cannot figure out a way out, then ask the user whatever. And once the task is actually done, you get an artifact back. So in this case, you would get an artifact back and now you know it\\'s done. Again, A2A allows you to follow up on artifacts as well. That\\'s another conversation. Check out the previous video. Cool. So the point I\\'m trying to make, support for collaboration. What does A2A say? Hell yeah, that is better. I\\'m just enjoying myself today. I\\'m having fun on this stream. What does MCP say? Yeah, but you got to do it yourself. Like you\\'ll have to architect the tools and stuff yourself. So it\\'s yeah with an asterk you can do it. It just it\\'s not as seamless as A2A. Okay. And this collaboration is a big one. Not just that this is something for me. So the point is whenever you have intelligent agent that means there\\'s an LLM somewhere in the middle, right? And again for a good experience you know if you\\'re returning the LLM\\'s output to the user you want to stream the output right so a good user experience requires streaming like as the response is being generated by the server you want to see it be typed like how we see it with chat GPT for that matter right so in that case what can happen is again MCP supports streaming just how you would stream with uh Win AI spec you know just say stream true and the best part is again it\\'s so freaking explicit because in your agent to agent card. The agent explicitly says I support streaming, right? Which is great. So instead of submitting a task now you say I want to submit subscribe and then you could just say hey this is something some response I want from you and now instead of saying completed the server sends back a status of working because again it was send subscribe instead of just send task and now you\\'re just getting your stream back and it\\'s so cool. Just look at it. It\\'s saying that okay the answer I\\'m going to give back is finally an artifact. So it\\'s not a follow-up question. It\\'s directly an artifact. This is my section. Don\\'t append this cuz remember when you use OpenAI streaming you get delta chunks back. So you need to like append it on your own end and you know things like that. But some servers might give you the entire response back. You don\\'t need to append it. So here the response is explicit. It\\'s saying that okay this chunk don\\'t append it. Meaning this chunk gets replaced. So whatever you had in your buffer replace it with this. And then when you get the next chunk you see here it\\'s append. So now what it\\'s saying is that this next section needs to be appended to your previous array. So you\\'re adding it again if the append was false in that case you would replace it with your previous array. It is so explicit. So it\\'s giving you best of both worlds. It\\'s giving you a place where if you want to stream delta you can stream delta and just the delta chunks and just say append true. If you want to stream the entire thing over and over again because maybe you\\'re streaming JSON and for JSON it\\'s better to stream the entire JSON over and over again. Then make append false. It supports both the scenarios and it\\'s so freaking explicit. I love it. And then it says obviously that hey this is the last chunk so we are done here right so it\\'s streaming but a problem with streaming is what if you had a network problem right so what happens then 8way supports resubscribing as well so you could say that hey you know what I disconnect from the previous task can I just resubscribe and it\\'s going to be like yeah sure I mean let\\'s resubscribe and it\\'s going to continue from where it left off now again the nuance here is that the server would have to like send the original chunk again so that\\'s a server nuance which is not really documented but I\\'m assuming most servers will start streaming everything again all over again, right? They won\\'t just continue from that instant. They will stream the previous thing again. I\\'m assuming or the server could be like, \"Oh, I remember that the last thing I sent you was this. So, I\\'m going to continue from here.\" Again, I would suggest that the server sends everything. So, the first message server sends back will be append false, which means replace the first chunk I\\'m sending you everything again and the subsequent ones will be append true. It\\'s so flexible. This is what a well thought of specification look like. So the answer yours is nope. We don\\'t support it. There\\'s no streaming support in this. Technically MCP does have support for something called notifications which could be used as streaming but it\\'s not a standard protocol. Right? Your client doesn\\'t know whether it supports streaming or not. The client doesn\\'t know if it\\'s going to give the response directly or it\\'s going to use notifications. There\\'s no clear indication on how notifications work. So I don\\'t think anybody\\'s going to be using it. So yeah, streaming I\\'m just going to call it a no. And A2A takes a hit in that one. And chances that an intelligence server requires streaming is high. Again, if you\\'re using intelligence to, you know, do a database call and return back the database call, you probably don\\'t need streaming. But if you\\'re returning back an LRM response, you do need streaming. I mean, I would argue even for a database call, like if this was an A2A server, you could stream back each result from the database one by one, right? You can keep streaming that one by one. So again, it looks good. I think I don\\'t know. I just think streaming is a very good thing for user experience. And talking about user experience, we need events. So in this entire thing, remember because our user is a monkey, we need to tell the user that, hey, something\\'s happening. So you can do that. So for example, you want to tell the user that, hey, I just got two tasks. Hey, you know what? I\\'m trying to schedule things now. Oh, we got a problem now. Hey, we\\'re trying to break it down. Hey, you know, it\\'s done. Giving updates to the user so that the user knows what the agent is doing. And obviously, if it\\'s an agent, it\\'s multi-art processes. It\\'s going to get the schema. It\\'s going to write to the database. There could be multiple steps involved, right? Keeping the user in loop of what is happening is really important. Like cursor, like cursor on the right gives you or you know wherever your agent tab is. On the agent tabs, it tells you, hey, I\\'m reading this file. I\\'m generating this code. It\\'s good to know what it\\'s doing. So if it\\'s going on the wrong track, you just cancel it, right? And yeah, A2A supports cancelling as well. So in A2A, you can technically cancel. I\\'m going to just add that as well. I\\'m so sorry. There\\'s just so many cool things about A2A, right? So like again in cursor if the agent is giving you events and you\\'re like hey uh the events don\\'t make any sense you can just cancel it. In MCP you cannot cancel squat. So MCP has no concept of steps. So if your this step was doing 10 steps under the hood it cannot send intermediate steps. I mean there is again notifications but it\\'s really complicated to do that. So I\\'m not going to talk about that. So you have first class support for events which means if you want to cancel it midway you can actually even cancel it midway. It\\'s just easier to do. I\\'m just going to write it over here. The last thing I want to just highlight is so yeah it has events and the server can explicitly say you know give it back events saying that hey I\\'m doing this I\\'m doing this I\\'m doing this the spec already covers that and it\\'s also explicit whether the server supports doing that and as a client you can also say that I\\'m not interested in those events so you don\\'t have to receive those events you can request for those events and you can even see if the server supports sending events right so it\\'s very flexible that ways you can cancel running task you can do that as well you also calls something called notifications which is a really cool feature wherein you can say that any agent was trying to create a YouTube video and it takes like 2 days to make it because it\\'s a longunning process. So A2A has first class concept to say that you know what send a web hook with the video right. So technically A2A can send a web hook whenever an artifact is ready technically as a part of the protocol. So the client can say that hey you know what I know you\\'re going to take a lot of time so don\\'t give me the response of the task here I don\\'t care about it okay just take this task do whatever you want and I\\'m going away and the client can say once you\\'re done and the client will before going away it will be like hey server before I go remember once you\\'re done with the artifact can you give it back to me on this URL and the server will remember it and as soon as the task is done the server will call that uh URL so the client can explicitly tell the user to submit task status changes through web hook in MCP P I mean this is custom. You can do it but the protocol doesn\\'t support it. So I\\'m just going to call it a no. I mean but you can build it yourself. You could have a resource where you say that okay all artifacts should go to this URL or something like that. You can use resource and things like that to kind of configure these kind of details. Again it\\'s just you have to do a lot more work right hopefully that makes sense. Okay there is a lot of stuff here. A28 talk to other agents collaborate. Yes MCP use tools precisely. I think that\\'s the perfect place to even end things, right? So the conclusion would be A2A allows for intelligence on the server. So whenever you want to create a server and you think you\\'re going to use an LLM there, A2A would be a good bet. You can do it with MCP, but eventually you run into this problems like what if you want to collaborate? What if you want to ask user follow-up questions? What if you want to stream? What if you want to have events? MCP currently does not support those things. So if you\\'re having an LLM on it server on an MCP server, you\\'re going to lose out on this features which is a little annoying. Away does that really well, but A2A is a complex protocol. So if you just have a server which does things like these, A2A would be an overkill, especially the fact that it can\\'t even support these things. Let me know how are you planning to use A2A. Let me know if you agree with what I said, right? U especially when you plan to use A2A and when you plan to use MCP. I just want to know different use cases. And remember, I stream every Tuesday. So I love to see you there. Like, share, and subscribe if you found this video to be helpful.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "833a912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text splitting\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 150)\n",
    "chunks = splitter.create_documents([transcript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1d82445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fc3281b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"I hope this makes sense. There's one more thing I want to highlight. Let's say in an MCP server, we have something called create task and it takes a project and a task. Okay, let's say I have an MCP server which takes two arguments. Now, what if the user's original question was like create this task and they do not mention a project. What would happen? Think about that. What would happen if the user shows intent to call a particular tool but doesn't give you all the inputs? In that case, your client, this guy would have to be smart enough to say that okay, to call this MCP tool, I need some more input. So, I need to ask for more input from the user. Right? The client would have to be smart saying that okay, you know what to call this MCB tool, I need two parameters. The user's question just has one parameter. So, I have to go back to the user and say that hey, you know, user uh you need to give me some more information. So there's a lot of intelligence which has to sit on the MCP\")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7d18dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding generation and storing in vector store\n",
    "embedding_model = OpenAIEmbeddings(model= 'text-embedding-3-small')\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cff725d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'aa4214ba-3537-42ce-a33f-2195152a5e94',\n",
       " 1: '3a4a8d3a-736e-4083-8307-dacceea8d26d',\n",
       " 2: 'b9e781cd-b15a-4739-8e04-94bc62380c07',\n",
       " 3: '861d3454-d64f-4ad0-aba9-1133ceb4ddcd',\n",
       " 4: '070607c6-060f-4cf3-b530-6a0b0560dcb3',\n",
       " 5: '40ffb8e6-57ef-4e4c-8eb0-4803cfe78f90',\n",
       " 6: 'f94b7366-638e-4622-a9d4-7c46c1d653c8',\n",
       " 7: 'c05102b1-42ee-4119-be1f-4899975fe9b2',\n",
       " 8: '98467f12-944b-4cf8-bd4d-85834ca4f751',\n",
       " 9: '0f61133a-8d3c-4e15-a0a5-9dc50eb42448',\n",
       " 10: '7c24570b-daaa-4071-a9f9-f94a09b29eb2',\n",
       " 11: '46d1de4d-7201-4c99-b669-e63ca64bbb81',\n",
       " 12: 'd8148c43-b38c-44f5-ad24-b0ab87d6a454',\n",
       " 13: '028ee637-48d1-4242-8365-9d3cdd7722d7',\n",
       " 14: '93ea5cfd-addf-46ca-8131-922b23de8a1e',\n",
       " 15: 'd7152eb6-093d-4a56-9873-750d4c9ca001',\n",
       " 16: '48fb72ee-3229-4a22-b40e-8cee9121d7d1',\n",
       " 17: 'f1955954-ad9f-4ec1-a71a-5342edf13130',\n",
       " 18: '3a06fb33-80a7-4b40-a637-18b9a6745d47',\n",
       " 19: '71921346-142c-4e59-b037-2a4cb65d4492',\n",
       " 20: 'c9818f92-5a81-4ef6-9968-5354f8361ebe',\n",
       " 21: 'aef0c5e8-6fd8-4cf9-95be-e07475d48317',\n",
       " 22: 'd5dbafb2-e434-4280-b92c-4b386737850f',\n",
       " 23: '37b15d49-07b6-4127-acff-2ad49797a7cb',\n",
       " 24: 'e23a851a-666a-4d7b-a9f0-0d750f6ca5d4',\n",
       " 25: '890998ba-1e0c-4b5a-9f6d-5b9c6d7e20da',\n",
       " 26: '80ab1902-f384-4c5c-a6a3-1508b31c4ccc',\n",
       " 27: 'f175a1ad-c6d9-4ec7-a76a-9fbd50a834e0',\n",
       " 28: 'fdcef321-cf41-4cee-bd2d-3906cb583c58',\n",
       " 29: 'ea1989d1-72a5-4ff9-b638-1d7671a3c5ec',\n",
       " 30: '83eceb41-d447-491d-b073-dc8b826ac9b5',\n",
       " 31: 'b4e3ed33-d530-4025-9607-e80798b124c3',\n",
       " 32: '12303472-cd26-4421-add0-d91b29c34470',\n",
       " 33: '11e2ce44-0682-44c8-87a6-2ddb81a08bb7',\n",
       " 34: 'd3dd83c5-a2a4-439f-acc1-4cd88bcacfb2',\n",
       " 35: 'e7c1d3de-41ac-4322-bd96-00354b6e5f34',\n",
       " 36: '04df706c-6124-49bc-8a24-18ff8163cbdd',\n",
       " 37: '37315679-8cfc-4cd3-9b05-b8b40d8013c3',\n",
       " 38: '0822b01b-26ff-4114-bc7e-e4151c0640f4',\n",
       " 39: 'b4dfcd5f-9a72-4296-9ce6-66bd7a126ddc',\n",
       " 40: '6c654509-83b1-4009-8f2e-5d0a7c0686a8',\n",
       " 41: '10be03d4-c9c4-4dac-a0fe-de72d5958a60',\n",
       " 42: '7ab3a0f0-02c7-49dc-a751-16e2884cc5f6',\n",
       " 43: 'a3b168c7-0cd2-498a-8e87-7c893e50b488',\n",
       " 44: '2acfcc5c-a349-47db-aa11-3967c7aec078',\n",
       " 45: '99da6e14-c48f-4c46-ac02-fe674c1d488d',\n",
       " 46: 'bf63a79e-c94e-436d-810b-00eacecd1c84',\n",
       " 47: '1f185ffe-10e7-462e-872f-8f043eb65859',\n",
       " 48: '05937004-0fa2-40fa-8acd-b69d5ef497a5',\n",
       " 49: 'b78987bf-2a7d-4beb-956d-c024a85f2fd7',\n",
       " 50: 'a7eaa638-8229-46bd-be87-f185a38ed0aa',\n",
       " 51: 'faa8f893-b88b-4aae-b96b-542687ff52cb',\n",
       " 52: '48a419a3-139e-4a90-adcf-1477424a9b2d',\n",
       " 53: '720a12a5-d371-4a23-b12e-134a8a1c77ad',\n",
       " 54: '543902d9-e9a2-4936-a35d-a2c8b6674020'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfc4220b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='720a12a5-d371-4a23-b12e-134a8a1c77ad', metadata={}, page_content=\"A28 talk to other agents collaborate. Yes MCP use tools precisely. I think that's the perfect place to even end things, right? So the conclusion would be A2A allows for intelligence on the server. So whenever you want to create a server and you think you're going to use an LLM there, A2A would be a good bet. You can do it with MCP, but eventually you run into this problems like what if you want to collaborate? What if you want to ask user follow-up questions? What if you want to stream? What if you want to have events? MCP currently does not support those things. So if you're having an LLM on it server on an MCP server, you're going to lose out on this features which is a little annoying. Away does that really well, but A2A is a complex protocol. So if you just have a server which does things like these, A2A would be an overkill, especially the fact that it can't even support these things. Let me know how are you planning to use A2A. Let me know if you agree with what I said, right? U\")]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.get_by_ids(['720a12a5-d371-4a23-b12e-134a8a1c77ad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98e17e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieval \n",
    "retriever = vector_store.as_retriever(search_type = 'similarity', search_kwargs = {\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4efc3d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002547FBF3BE0>, search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21ca39f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='48fb72ee-3229-4a22-b40e-8cee9121d7d1', metadata={}, page_content=\"in MCP which says send task and the message could be any. It could be string, it could be binary, it could be anything. And now you have the same kind of API for both of them, right? So A2A is kind of like MCP with a single tool. That's what I'm trying to highlight. A2A is like MCP server with a single tool where you just take a message as the input and the message could be anything. It could be natural language string. It could be structured. It could be a JSON object. It could literally be anything. However, most people will use MCP more like this like get tasks uh create task and stuff like that, right? Create task and maybe you have a task as well, whatever. You get the point, guys. Again, A2A is like an MCP server with just a single tool. All right? However, in MCP, the spec currently at least currently only has a description. So, the only way you can describe what this tool does is through a description. So you would have to write like a really huge description with a lot of\"),\n",
       " Document(id='b4e3ed33-d530-4025-9607-e80798b124c3', metadata={}, page_content=\"do it, but mostly it's going to be strict, right? Because you're defining the freaking type of every parameter you want, right? So, it's as strict as it gets. This is a function call. MCP is just RPC. It's remote procedural calls. Okay? Now, we're going to get into the big stuff. So, right now, I've just been stating some lowhanging fruits. I'll get into justifying this now. So, now comes a little a heavier portion. So, I want to take a pause here. Uh now I want to get into some big differences on why A2A servers should be intelligent. So I want to focus on this why intelligent servers should be A2A and why dumb servers should be MCP. Okay, that's what we need to get at. Let's start with the obvious point which we've already covered. I'll just reiterate it. If you have an MCP for notion, right, which does create get task and create task or whatever, somebody has to make a plan. Let's take a simpler example which I think a lot of people have seen out in the world. Let's say you have an\"),\n",
       " Document(id='d7152eb6-093d-4a56-9873-750d4c9ca001', metadata={}, page_content=\"I don't want to make you think that wait that A2A is more broad and MCP cannot be broad. MCP can be broad as well. Let's just uh drill down on this a little bit more. Okay. So basically what I'm suggesting is in MCP you have like get tasks right and then for get tax you could take a project as an input and you give some output right that's an example of how the tool would look like and technically what I'm saying here that this will only have one function let's call it uh send task and this could be anything this is just a message which could be any and by any I mean obviously there's some protocols around it it could be a string it could be a image it could be a lot of different things but that's what I'm saying right but technically in MCP as well you could have the same kind of a function, right? You could technically just have one tool in MCP which says send task and the message could be any. It could be string, it could be binary, it could be anything. And now you have the same\"),\n",
       " Document(id='11e2ce44-0682-44c8-87a6-2ddb81a08bb7', metadata={}, page_content='what your MCP server is you\\'ll always need some kind of intelligence now like also we discussed you could have something like this you could have an MCP tool which is essentially run natural language query in this it seems like the client is becoming dumb again right like it can just do this correct uh it can just take the natural language and pass this and this works Don\\'t get me wrong, this works. You could have an MCP server which, you know, gets the schema, does all that fun stuff and, you know, then calls the database. So, yeah, you could technically have an MCP server which does this, but once you start getting into the real world examples, right, and you figure out something like the user\\'s asking a question about tasks, but there is no table called tasks. Now, this tool will suddenly give you a response back saying, \"Wait a minute. Know what table you\\'re talking about? Did you mean this?\" Right? Something like that. Wait a minute. I don\\'t know what you\\'re talking about. Did'),\n",
       " Document(id='890998ba-1e0c-4b5a-9f6d-5b9c6d7e20da', metadata={}, page_content=\"and what would this MCP server do this MCP server would have some tools Google search knowledgebased search that's it that's what my MCP server would do I would have an A2A server saying I have a skill cuz remember we have skills here so I'm just writing it out so we have a skill uh MCP server tools and A2A server skills I would say I would have an A2A server with skills and I would say my skill is what I do research and I've got some examples find me research papers on something from Google right you could have another example saying again remember examples is a part of the official spec okay I'm not making things up this is part of the spec I could have another example answer this question from my company knowledge base you see the difference here so over here I'm saying hey this is a skill I do research I have these sample things I can search from Google I can search from knowledgebased and this server internally would be using MCP tools to do the Google search and knowledgebased\")]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is MCP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "904a3e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmentation\n",
    "llm = ChatOpenAI(temperature= 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d09ddec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template = \"\"\"You are a helpful assistant.\n",
    "      Answer ONLY from the provided transcript context.\n",
    "      If the context is insufficient, just say you don't know.\n",
    "\n",
    "      {context}\n",
    "      Question: {question}\"\"\",\n",
    "      input_variables= ['context', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52fe9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is the topic of AI Agent discussed in this video?\"\n",
    "retrieved_docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67e19be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='aa4214ba-3537-42ce-a33f-2195152a5e94', metadata={}, page_content=\"Okay, Google just released its agent to aagent protocol, but with its launch, there are some big questions which arise like what about MCP? Like does A2A replace MCP? Is there an overlap? And if not, when do you use what? Well, in this stream, we're going to pit A2A against MCP and see what their differences are. Take a good example to highlight some use cases and finally arrive at a framework which helps us decide when to use what. Let's get started. I think the best way to compare any frameworks is by looking at an example, right? And we already saw this, so let's just go through it again. It can be a quick recap. Okay, let's say that I'm a human. Yeah, a cute little guy. That's me. Yep. Right here. And let's say I'm supposed to finish a project by the end of the week, right? And I keep all my project stuff in notion and I keep all my calendar in Google calendar, right? So my projects and my tasks are in notion and everything else is in Google calendar. And I just want to make sure\"),\n",
       " Document(id='070607c6-060f-4cf3-b530-6a0b0560dcb3', metadata={}, page_content=\"know that okay my agent is actually doing the right thing which is kind of important and not just that I mean you could even have some human and loop scenarios right where you could be like okay we want to break a task down and notion was like yeah you can break this task down but what if notion doesn't know how to break the task down what if it comes up with incorrect subtasks maybe we want to show the user an input hey we want to break task B down can we do that and the user goes like yeah cool you can do this and then you schedule it so You have like a a little human in loop scenario going on in here. That's how the real world operates, right? Your agents might screw things up or some things might not be possible. So, you need like follow-up questions. And then you also want to give feedback to the user. You also want like human and loop scenarios and all that funny stuff. That's how real agent interaction works, at least in the real world, right? This is the example we're going to\"),\n",
       " Document(id='7ab3a0f0-02c7-49dc-a751-16e2884cc5f6', metadata={}, page_content=\"user whatever. And once the task is actually done, you get an artifact back. So in this case, you would get an artifact back and now you know it's done. Again, A2A allows you to follow up on artifacts as well. That's another conversation. Check out the previous video. Cool. So the point I'm trying to make, support for collaboration. What does A2A say? Hell yeah, that is better. I'm just enjoying myself today. I'm having fun on this stream. What does MCP say? Yeah, but you got to do it yourself. Like you'll have to architect the tools and stuff yourself. So it's yeah with an asterk you can do it. It just it's not as seamless as A2A. Okay. And this collaboration is a big one. Not just that this is something for me. So the point is whenever you have intelligent agent that means there's an LLM somewhere in the middle, right? And again for a good experience you know if you're returning the LLM's output to the user you want to stream the output right so a good user experience requires\"),\n",
       " Document(id='0f61133a-8d3c-4e15-a0a5-9dc50eb42448', metadata={}, page_content=\"we're just going to limit our thing to tools and skills because I think that's where a lot of the mixup would happen. Prompts and resources is like a completely different thing. So, I'm just going to skip those things. So, in A2A, you have the concept of a skill. In MCP, you have a concept of a tool. So we already saw an example for A2A but let's go through that again. Let's just look at discovery first and then we can say who can use it right. So the first how it works for A2A. You have something called the agent card and this agent card is on steroids. This is good stuff. This is what you can discover. So every agent publishes this card. It's just a JSON object and it declares that hey this is my capability right. So you can clearly see that okay as input it supports plain text and as output it can do plain text as well and it can even create HTML code as output. In capabilities it says it can support streaming. So it supports streaming responses. It supports something called push\"),\n",
       " Document(id='d8148c43-b38c-44f5-ad24-b0ab87d6a454', metadata={}, page_content=\"very high level broad capabilities or queries you can ask this agent right and it's very natural language focused I think that's a really important point again I don't know how to best describe this I'm so sorry about that but in A2A skills are really broad capabilities with some examples on how you can mess around with it there is intelligence on the back end which is trying its best to take your query and give you an answer on it so under the hood technically speaking this one thing could require 10 tool calls maybe. I don't know how many tool calls would Google have to do to answer this. I don't know how many tool calls would Google have to do to answer that. The point is I just give it questions in natural language and I don't have to worry about the breaking down of questions and things like that. I can just give it questions and give me some example questions as well. Thank you. And obviously you can have as many skills as you want. So that's how you define skills in A2A at\")]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d45dd9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, Google just released its agent to aagent protocol, but with its launch, there are some big questions which arise like what about MCP? Like does A2A replace MCP? Is there an overlap? And if not, when do you use what? Well, in this stream, we're going to pit A2A against MCP and see what their differences are. Take a good example to highlight some use cases and finally arrive at a framework which helps us decide when to use what. Let's get started. I think the best way to compare any frameworks is by looking at an example, right? And we already saw this, so let's just go through it again. It can be a quick recap. Okay, let's say that I'm a human. Yeah, a cute little guy. That's me. Yep. Right here. And let's say I'm supposed to finish a project by the end of the week, right? And I keep all my project stuff in notion and I keep all my calendar in Google calendar, right? So my projects and my tasks are in notion and everything else is in Google calendar. And I just want to make sure\\n\\nknow that okay my agent is actually doing the right thing which is kind of important and not just that I mean you could even have some human and loop scenarios right where you could be like okay we want to break a task down and notion was like yeah you can break this task down but what if notion doesn't know how to break the task down what if it comes up with incorrect subtasks maybe we want to show the user an input hey we want to break task B down can we do that and the user goes like yeah cool you can do this and then you schedule it so You have like a a little human in loop scenario going on in here. That's how the real world operates, right? Your agents might screw things up or some things might not be possible. So, you need like follow-up questions. And then you also want to give feedback to the user. You also want like human and loop scenarios and all that funny stuff. That's how real agent interaction works, at least in the real world, right? This is the example we're going to\\n\\nuser whatever. And once the task is actually done, you get an artifact back. So in this case, you would get an artifact back and now you know it's done. Again, A2A allows you to follow up on artifacts as well. That's another conversation. Check out the previous video. Cool. So the point I'm trying to make, support for collaboration. What does A2A say? Hell yeah, that is better. I'm just enjoying myself today. I'm having fun on this stream. What does MCP say? Yeah, but you got to do it yourself. Like you'll have to architect the tools and stuff yourself. So it's yeah with an asterk you can do it. It just it's not as seamless as A2A. Okay. And this collaboration is a big one. Not just that this is something for me. So the point is whenever you have intelligent agent that means there's an LLM somewhere in the middle, right? And again for a good experience you know if you're returning the LLM's output to the user you want to stream the output right so a good user experience requires\\n\\nwe're just going to limit our thing to tools and skills because I think that's where a lot of the mixup would happen. Prompts and resources is like a completely different thing. So, I'm just going to skip those things. So, in A2A, you have the concept of a skill. In MCP, you have a concept of a tool. So we already saw an example for A2A but let's go through that again. Let's just look at discovery first and then we can say who can use it right. So the first how it works for A2A. You have something called the agent card and this agent card is on steroids. This is good stuff. This is what you can discover. So every agent publishes this card. It's just a JSON object and it declares that hey this is my capability right. So you can clearly see that okay as input it supports plain text and as output it can do plain text as well and it can even create HTML code as output. In capabilities it says it can support streaming. So it supports streaming responses. It supports something called push\\n\\nvery high level broad capabilities or queries you can ask this agent right and it's very natural language focused I think that's a really important point again I don't know how to best describe this I'm so sorry about that but in A2A skills are really broad capabilities with some examples on how you can mess around with it there is intelligence on the back end which is trying its best to take your query and give you an answer on it so under the hood technically speaking this one thing could require 10 tool calls maybe. I don't know how many tool calls would Google have to do to answer this. I don't know how many tool calls would Google have to do to answer that. The point is I just give it questions in natural language and I don't have to worry about the breaking down of questions and things like that. I can just give it questions and give me some example questions as well. Thank you. And obviously you can have as many skills as you want. So that's how you define skills in A2A at\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03d3e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = prompt.invoke({\"context\": context_text, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acb33be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"You are a helpful assistant.\\n      Answer ONLY from the provided transcript context.\\n      If the context is insufficient, just say you don't know.\\n\\n      Okay, Google just released its agent to aagent protocol, but with its launch, there are some big questions which arise like what about MCP? Like does A2A replace MCP? Is there an overlap? And if not, when do you use what? Well, in this stream, we're going to pit A2A against MCP and see what their differences are. Take a good example to highlight some use cases and finally arrive at a framework which helps us decide when to use what. Let's get started. I think the best way to compare any frameworks is by looking at an example, right? And we already saw this, so let's just go through it again. It can be a quick recap. Okay, let's say that I'm a human. Yeah, a cute little guy. That's me. Yep. Right here. And let's say I'm supposed to finish a project by the end of the week, right? And I keep all my project stuff in notion and I keep all my calendar in Google calendar, right? So my projects and my tasks are in notion and everything else is in Google calendar. And I just want to make sure\\n\\nknow that okay my agent is actually doing the right thing which is kind of important and not just that I mean you could even have some human and loop scenarios right where you could be like okay we want to break a task down and notion was like yeah you can break this task down but what if notion doesn't know how to break the task down what if it comes up with incorrect subtasks maybe we want to show the user an input hey we want to break task B down can we do that and the user goes like yeah cool you can do this and then you schedule it so You have like a a little human in loop scenario going on in here. That's how the real world operates, right? Your agents might screw things up or some things might not be possible. So, you need like follow-up questions. And then you also want to give feedback to the user. You also want like human and loop scenarios and all that funny stuff. That's how real agent interaction works, at least in the real world, right? This is the example we're going to\\n\\nuser whatever. And once the task is actually done, you get an artifact back. So in this case, you would get an artifact back and now you know it's done. Again, A2A allows you to follow up on artifacts as well. That's another conversation. Check out the previous video. Cool. So the point I'm trying to make, support for collaboration. What does A2A say? Hell yeah, that is better. I'm just enjoying myself today. I'm having fun on this stream. What does MCP say? Yeah, but you got to do it yourself. Like you'll have to architect the tools and stuff yourself. So it's yeah with an asterk you can do it. It just it's not as seamless as A2A. Okay. And this collaboration is a big one. Not just that this is something for me. So the point is whenever you have intelligent agent that means there's an LLM somewhere in the middle, right? And again for a good experience you know if you're returning the LLM's output to the user you want to stream the output right so a good user experience requires\\n\\nwe're just going to limit our thing to tools and skills because I think that's where a lot of the mixup would happen. Prompts and resources is like a completely different thing. So, I'm just going to skip those things. So, in A2A, you have the concept of a skill. In MCP, you have a concept of a tool. So we already saw an example for A2A but let's go through that again. Let's just look at discovery first and then we can say who can use it right. So the first how it works for A2A. You have something called the agent card and this agent card is on steroids. This is good stuff. This is what you can discover. So every agent publishes this card. It's just a JSON object and it declares that hey this is my capability right. So you can clearly see that okay as input it supports plain text and as output it can do plain text as well and it can even create HTML code as output. In capabilities it says it can support streaming. So it supports streaming responses. It supports something called push\\n\\nvery high level broad capabilities or queries you can ask this agent right and it's very natural language focused I think that's a really important point again I don't know how to best describe this I'm so sorry about that but in A2A skills are really broad capabilities with some examples on how you can mess around with it there is intelligence on the back end which is trying its best to take your query and give you an answer on it so under the hood technically speaking this one thing could require 10 tool calls maybe. I don't know how many tool calls would Google have to do to answer this. I don't know how many tool calls would Google have to do to answer that. The point is I just give it questions in natural language and I don't have to worry about the breaking down of questions and things like that. I can just give it questions and give me some example questions as well. Thank you. And obviously you can have as many skills as you want. So that's how you define skills in A2A at\\n      Question: Is the topic of AI Agent discussed in this video?\")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e9cb475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the topic of AI Agent is discussed in this video, specifically comparing A2A and MCP frameworks.\n"
     ]
    }
   ],
   "source": [
    "answer = llm.invoke(final_prompt)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "efa29307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af265304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(retrieved_docs):\n",
    "    context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33300a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel({\n",
    "    'context': retriever | RunnableLambda(format_docs),\n",
    "    'question': RunnablePassthrough()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "262167ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': \"an artifact called order confirmation and you can have some details. If you were generating an image, the artifact would be the image. If you were creating a task, the task ID could be the artifact. The point is once you get the artifact, you know for sure that hey, things are done. I have an answer. Now I can be dumb and just show this to the user or I can use this to I make a decision, call another A2A agent or whatever. You see how explicit A2A is. The agent has first class capability to request for additional input. And we saw that in our example as well when we were trying to schedule some tasks with the calendar. Essentially what was happening is the calendar did not have a 4hour block free. So it asks a question. It sends back a message saying hey I need some more questions. Again the user could just you know take that and handle it directly. But in our case our assistance was intelligent. Again there's nothing wrong with intelligence right? the the assistance was intelligent\\n\\nthe user and say that hey, you know, user uh you need to give me some more information. So there's a lot of intelligence which has to sit on the MCP client and you know we'll talk about intelligence soon as intelligence is going to be a very big one. I hope you get it. The point I'm trying to convey is in case of MCP it's the client's responsibility to populate all the parameters to be called. In case of A2A you can literally pass it everything. You can even give it an incomplete input. You can technically still give it an incomplete input and it would still work cuz A2A has built-in mechanism where the agent will ask you for further clarification. Right? So your client doesn't have to worry about it so much. Okay. Important point. And the biggest point I want to highlight here is in case of A2A intelligence moves from the server to the client. So intelligence mostly on the server mostly. I'm not saying your client cannot be intelligent. I'm saying in most use cases like 80% of the\\n\\nvery high level broad capabilities or queries you can ask this agent right and it's very natural language focused I think that's a really important point again I don't know how to best describe this I'm so sorry about that but in A2A skills are really broad capabilities with some examples on how you can mess around with it there is intelligence on the back end which is trying its best to take your query and give you an answer on it so under the hood technically speaking this one thing could require 10 tool calls maybe. I don't know how many tool calls would Google have to do to answer this. I don't know how many tool calls would Google have to do to answer that. The point is I just give it questions in natural language and I don't have to worry about the breaking down of questions and things like that. I can just give it questions and give me some example questions as well. Thank you. And obviously you can have as many skills as you want. So that's how you define skills in A2A at\\n\\nthe user knows what the agent is doing. And obviously, if it's an agent, it's multi-art processes. It's going to get the schema. It's going to write to the database. There could be multiple steps involved, right? Keeping the user in loop of what is happening is really important. Like cursor, like cursor on the right gives you or you know wherever your agent tab is. On the agent tabs, it tells you, hey, I'm reading this file. I'm generating this code. It's good to know what it's doing. So if it's going on the wrong track, you just cancel it, right? And yeah, A2A supports cancelling as well. So in A2A, you can technically cancel. I'm going to just add that as well. I'm so sorry. There's just so many cool things about A2A, right? So like again in cursor if the agent is giving you events and you're like hey uh the events don't make any sense you can just cancel it. In MCP you cannot cancel squat. So MCP has no concept of steps. So if your this step was doing 10 steps under the hood it\\n\\nwe're just going to limit our thing to tools and skills because I think that's where a lot of the mixup would happen. Prompts and resources is like a completely different thing. So, I'm just going to skip those things. So, in A2A, you have the concept of a skill. In MCP, you have a concept of a tool. So we already saw an example for A2A but let's go through that again. Let's just look at discovery first and then we can say who can use it right. So the first how it works for A2A. You have something called the agent card and this agent card is on steroids. This is good stuff. This is what you can discover. So every agent publishes this card. It's just a JSON object and it declares that hey this is my capability right. So you can clearly see that okay as input it supports plain text and as output it can do plain text as well and it can even create HTML code as output. In capabilities it says it can support streaming. So it supports streaming responses. It supports something called push\",\n",
       " 'question': 'What is AI Agent'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_chain.invoke('What is AI Agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63b83de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "226e75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_chain = parallel_chain | prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4999980a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An AI agent is a software program that can perform tasks, make decisions, and interact with users using natural language. It can request additional input, handle incomplete information, and provide intelligent responses. In the context of A2A (Agent to Agent) communication, the intelligence of the agent is primarily on the server side, but in A2A, intelligence can also be on the client side. A2A agents have broad capabilities and can handle various queries in natural language.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_chain.invoke('what is an AI agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c8e824bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. A2A focuses on proving why a conclusion is better rather than just arriving at a conclusion.\\n2. The scope of A2A is not well defined due to the broad skills it possesses.\\n3. A2A supports streaming responses, push notifications, and task state updates.\\n4. A2A has skills like route planning, which are described with examples for clarity.\\n5. A2A's capability discovery is more organized and concise compared to MCP's tool descriptions.\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_chain.invoke('Summary of the video in 5 points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2827e18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
